{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality reduction\n",
    "**V.0.1 - Alpha testing, [contributions](#contributions)**\n",
    "\n",
    "fMRI data often has a dimensionality problem: we get approximately 100,000 voxels (i.e., features) per volume, but only 100s of time points or trials (i.e., examples). This makes it very hard for machine learning algorithms to model how each voxel contributes. For more general information on this problem, also dubbed the [curse of dimensionality](https://en.wikipedia.org/wiki/Curse_of_dimensionality), see [these slides from the Texas A&M University Computer Science and Engineering Department](http://courses.cs.tamu.edu/choe/11spring/633/lectures/slide08.pdf). For a neuroimaging-specific view on the curse of dimensionality, you might want to take a look at [Mwangi et al.'s Neuroinformatics review from 2014](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4040248/).\n",
    "\n",
    "## Goal of this script\n",
    "1. Learn to compute the covariance of a dataset.  \n",
    "2. Reduce the feature space using principal component analysis (PCA).\n",
    "3. Reduce the feature space using indipendent component analysis (ICA).\n",
    "4. Reduce the feature space using feature selection techniques.  \n",
    "5. Run classification on the dimensionality reduced feature space.  \n",
    "\n",
    "**Recap:** The localizer data we are working with ([Kim et al., 2017](https://doi.org/10.1523/JNEUROSCI.3272-16.2017)) consists of 3 runs with 5 blocks for each category. In the matlab stimulus file, the first row has the stimulus labels for the 1st, 2nd and 3rd runs of the localizer. Each run was 310 TRs.\n",
    "The 4th row contains the time when the stimulus was presented for each of the runs. The stimulus labels and their corresponding categories are as follows:  \n",
    "1 = Faces, 2 = Scenes, 3 = Objects\n",
    "\n",
    "## Table of Contents\n",
    "[1. Preprocessing](#preprocessing)  \n",
    "\n",
    "[2. Covariance](#covariance)  \n",
    "\n",
    "[3. PCA](#pca)  \n",
    ">[3.1 Plot PCA](#plot_pca)  \n",
    "\n",
    "[4. ICA](#ica)  \n",
    ">[3.1 Plot ICA](#plot_ica)  \n",
    "\n",
    "[5. Dimensionality Reduction](#dim_red)  \n",
    ">[5.1 Feature Selection: ROI](#roi)  \n",
    ">[5.2 Feature Selection: Top N voxels](#top_n)  \n",
    ">[5.3 Feature Selection: Variance Threshold](#var_thresh)  \n",
    ">[5.4 Feature Selection: Univariate](#univariate)  \n",
    ">[5.5 Feature Selection: Recursive Feature Elimination](#rfe)  \n",
    "\n",
    "Exercises\n",
    ">[Exercise 1](#ex1)  \n",
    ">[Exercise 2](#ex2)  \n",
    ">[Exercise 3](#ex3)  \n",
    ">[Exercise 4](#ex4)  \n",
    ">[Exercise 5](#ex5)  \n",
    ">[Exercise 6](#ex6)  \n",
    ">[Exercise 7](#ex7)  \n",
    ">[Exercise 8](#ex8)  \n",
    ">[Exercise 9](#ex9)  \n",
    ">[Exercise 10](#ex10)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import sys \n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    \n",
    "# Import neuroimaging, analysis and general libraries\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "from scipy import stats\n",
    "from time import time\n",
    "import pandas as pd\n",
    "\n",
    "# Import plotting libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "# %matplotlib notebook\n",
    "\n",
    "# Machine learning libraries\n",
    "from nilearn.input_data import NiftiMasker\n",
    "from nilearn.masking import compute_epi_mask\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import cross_val_score, StratifiedShuffleSplit \n",
    "from sklearn.svm import SVC\n",
    "from sklearn.decomposition import PCA, FastICA\n",
    "from sklearn.feature_selection import SelectKBest, VarianceThreshold, RFECV, chi2, SelectPercentile\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import datasets\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.set_autosave_interval(5000)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosaving every 5 seconds\n",
      "data dir = /home/pytorch51/public_FMRI/vdc/\n",
      "ROIs = ['FFA', 'PPA']\n",
      "Labels = {1: 'Faces', 2: 'Scenes', 3: 'Objects'}\n",
      "number of runs = 3\n"
     ]
    }
   ],
   "source": [
    "from qutils import vdc_data_dir, vdc_all_ROIs, vdc_label_dict, vdc_n_runs\n",
    "\n",
    "%matplotlib inline \n",
    "%autosave 5\n",
    "sns.set(style = 'white', context='poster', rc={\"lines.linewidth\": 2.5})\n",
    "\n",
    "print('data dir = %s' % (vdc_data_dir))\n",
    "print('ROIs = %s' % (vdc_all_ROIs))\n",
    "print('Labels = %s' % (vdc_label_dict))\n",
    "print('number of runs = %s' % (vdc_n_runs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preprocessing <a id=\"preprocessing\"></a>\n",
    "\n",
    "Let's set up some functions that we will be using in this script. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a function to load the mask data\n",
    "def load_data(directory, subject_name, mask_name='', num_runs=3, zscore_data=False):\n",
    "    \n",
    "    maskdir = (directory + subject_name + \"/preprocessed/masks/\")\n",
    "\n",
    "    # Cycle through the masks\n",
    "    print (\"Processing Start ...\")\n",
    "    \n",
    "    # If there is a mask supplied then load it now\n",
    "    if mask_name is not '':\n",
    "        maskfile = (maskdir + \"%s_ventral_%s_locColl_to_epi1.nii.gz\" % (subject_name, mask_name))\n",
    "\n",
    "        mask = nib.load(maskfile)\n",
    "        print (\"Loaded %s mask\" % (mask_name))\n",
    "\n",
    "    # Cycle through the runs\n",
    "    for run in range(1, num_runs + 1):\n",
    "        epi_in = (directory + subject_name + \"/preprocessed/loc/%s_filtered2_d1_firstExampleFunc_r%d.nii\" % (subject_name, run))\n",
    "        print(\"\\t\" + epi_in)\n",
    "\n",
    "        # Load in the fmri data\n",
    "        epi_data = nib.load(epi_in)\n",
    "        \n",
    "        # Mask the data if necessary\n",
    "        if mask_name is not '':\n",
    "            nifti_masker = NiftiMasker(mask_img=mask)\n",
    "            epi_mask_data = nifti_masker.fit_transform(epi_data);\n",
    "            epi_mask_data = np.transpose(epi_mask_data)\n",
    "        else:\n",
    "            # Do a whole brain mask \n",
    "            if run == 1:\n",
    "                mask = compute_epi_mask(epi_data).get_data()  # Compute mask from epi\n",
    "            else:\n",
    "                mask *= compute_epi_mask(epi_data).get_data()  # Get the intersection mask (set voxels that are within the mask on all runs to 1, set all other voxels to 0)   \n",
    "            \n",
    "            # Reshape all of the data from 4D (X*Y*Z*time) to 2D (voxel*time): not great for memory\n",
    "            epi_mask_data = epi_data.get_data().reshape(mask.shape[0] * mask.shape[1] * mask.shape[2], epi_data.shape[3])\n",
    "\n",
    "        # Transpose and z-score (standardize) the data  \n",
    "        if zscore_data == True:\n",
    "            scaler = preprocessing.StandardScaler().fit(epi_mask_data)\n",
    "            preprocessed_data = scaler.transform(epi_mask_data)\n",
    "        else:\n",
    "            preprocessed_data = epi_mask_data\n",
    "        \n",
    "        # Concatenate the data\n",
    "        if run == 1:\n",
    "            concatenated_data = preprocessed_data\n",
    "        else:\n",
    "            concatenated_data = np.hstack((concatenated_data, preprocessed_data))\n",
    "    \n",
    "    # Apply the whole-brain masking: First, reshape the mask from 3D (X*Y*Z) to 1D (voxel). \n",
    "    # Second, get indices of non-zero voxels, i.e. voxels inside the mask. \n",
    "    # Third, zero out all of the voxels outside of the mask.\n",
    "    if mask_name is '':\n",
    "        mask_vector = np.nonzero(mask.reshape(mask.shape[0] * mask.shape[1] * mask.shape[2], ))[0]\n",
    "        concatenated_data = concatenated_data[mask_vector, :]\n",
    "        \n",
    "    # Return the list of mask data\n",
    "    return concatenated_data\n",
    "\n",
    "# Make a function for loading in the labels\n",
    "def load_labels(directory, subject_name):\n",
    "    stim_label = [];\n",
    "    stim_label_concatenated = [];\n",
    "    for run in range(1,4):\n",
    "        in_file= (directory + subject_name + '/ses-day2/design_matrix/' + \"%s_localizer_0%d.mat\" % (subject_name, run))\n",
    "\n",
    "        # Load in data from matlab\n",
    "        stim_label = scipy.io.loadmat(in_file);\n",
    "        stim_label = np.array(stim_label['data']);\n",
    "\n",
    "        # Store the data\n",
    "        if run == 1:\n",
    "            stim_label_concatenated = stim_label;\n",
    "        else:       \n",
    "            stim_label_concatenated = np.hstack((stim_label_concatenated, stim_label))\n",
    "\n",
    "    print(\"Loaded \", subject_name)\n",
    "    return stim_label_concatenated\n",
    "\n",
    "\n",
    "# Convert the TR\n",
    "def label2TR(stim_labels, num_runs, TR, TRs_run):\n",
    "    \n",
    "    # Calculate the number of events/run\n",
    "    _, events = stim_labels.shape\n",
    "    events_run = int(events / num_runs)\n",
    "    \n",
    "    # Preset the array with zeros\n",
    "    stim_label_TR = np.zeros((TRs_run * 3, 1))\n",
    "\n",
    "    # Cycle through the runs\n",
    "    for run in range(0, num_runs):\n",
    "\n",
    "        # Cycle through each element in a run\n",
    "        for i in range(events_run):\n",
    "\n",
    "            # What element in the concatenated timing file are we accessing\n",
    "            time_idx = run * (events_run) + i\n",
    "\n",
    "            # What is the time stamp\n",
    "            time = stim_labels[2, time_idx]\n",
    "\n",
    "            # What TR does this timepoint refer to?\n",
    "            TR_idx = int(time / TR) + (run * (TRs_run - 1))\n",
    "\n",
    "            # Add the condition label to this timepoint\n",
    "            stim_label_TR[TR_idx]=stim_label_allruns[0, time_idx]\n",
    "        \n",
    "    return stim_label_TR\n",
    "\n",
    "# Create a function to shift the size\n",
    "def shift_timing(label_TR, TR_shift_size):\n",
    "    \n",
    "    # Create a short vector of extra zeros\n",
    "    zero_shift = np.zeros((TR_shift_size, 1))\n",
    "\n",
    "    # Zero pad the column from the top\n",
    "    label_TR_shifted = np.vstack((zero_shift, label_TR))\n",
    "\n",
    "    # Don't include the last rows that have been shifted out of the time line\n",
    "    label_TR_shifted = label_TR_shifted[0:label_TR.shape[0],0]\n",
    "    \n",
    "    return label_TR_shifted\n",
    "\n",
    "\n",
    "# Extract bold data for non-zero labels\n",
    "def reshape_data(label_TR_shifted, masked_data_all):\n",
    "    label_index = np.nonzero(label_TR_shifted)\n",
    "    label_index = np.squeeze(label_index)\n",
    "    \n",
    "    # Pull out the indexes\n",
    "    indexed_data = np.transpose(masked_data_all[:,label_index])\n",
    "    nonzero_labels = label_TR_shifted[label_index] \n",
    "    \n",
    "    return indexed_data, nonzero_labels\n",
    "\n",
    "# Run a basic n fold classification\n",
    "def classification(classifier, data, labels, n_folds=3, test_size=0.1):\n",
    "    \n",
    "    # How many folds of the classifier\n",
    "    skfold = StratifiedShuffleSplit(n_splits=n_folds, test_size=test_size) \n",
    "\n",
    "    clf_score = np.array([])\n",
    "    for train, test in skfold.split(data, labels):\n",
    "\n",
    "        # Pull out the sample data\n",
    "        train_data = data[train, :]\n",
    "        test_data = data[test, :]\n",
    "        \n",
    "        # Train and test the classifier\n",
    "        clf = classifier.fit(train_data, labels[train])\n",
    "        clf_score = np.hstack((clf_score, clf.score(test_data, labels[test])))\n",
    "\n",
    "    return clf_score.mean()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Self-study:** We use the method .get_data() on Nifti files in the load_data above. Find out why we need this method or what this method does in the [NiBabel Documentation](http://nipy.org/nibabel/gettingstarted.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The way we analyzed the data in week 3 was problematic because it assumed that each observation was independent. However, note that each trial within a block occurred within 1.5s of each other. In other words, there was substantial overlap in the HRF between adjacent trials. The autocorrelation in fMRI data means that adjacent time points will be similar despite potential variability in the underlying signal.\n",
    "\n",
    "One way to appropriately deal with this problem of independence is to treat blocks, rather than stimulus events, as our inputs in these analyses.\n",
    "\n",
    "**Exercise 1:** <a id=\"ex1\"></a> In the cell below, finish making a function that converts bold_data (the output of reshape_data) and labels into blockwise data, rather than eventwise data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take in a brain volume and label vector that is the length of the event number and convert it into a list the length of the block number\n",
    "def blockwise_sampling(eventwise_data, eventwise_labels, events_per_block=10):\n",
    "    \n",
    "    # How many events are expected\n",
    "    expected_blocks = int(eventwise_data.shape[0] / events_per_block)\n",
    "    \n",
    "    # Average the BOLD data for each block of trials\n",
    "    blockwise_data = np.zeros((expected_blocks, eventwise_data.shape[1]))\n",
    "    for block in list(range(expected_blocks)):\n",
    "        blockwise_data[block, :] = np.mean(eventwise_data[block * events_per_block : (block + 1) * events_per_block, :], axis=0)\n",
    "    \n",
    "    # Down sample labels\n",
    "    blockwise_labels = eventwise_labels[::events_per_block]\n",
    "    \n",
    "    # Check labels were correctly down sampled\n",
    "    \n",
    "    # Report the new variable sizes\n",
    "    print('Expected blocks: %d; Resampled blocks: %d' % (expected_blocks, blockwise_data.shape[0]))\n",
    "    \n",
    "    # Return the variables downsampled_data and downsampled_labels\n",
    "    return blockwise_data, blockwise_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Start ...\n",
      "\t/home/pytorch51/public_FMRI/vdc/sub-01/preprocessed/loc/sub-01_filtered2_d1_firstExampleFunc_r1.nii\n",
      "\t/home/pytorch51/public_FMRI/vdc/sub-01/preprocessed/loc/sub-01_filtered2_d1_firstExampleFunc_r2.nii\n"
     ]
    }
   ],
   "source": [
    "directory = vdc_data_dir\n",
    "subject_name = 'sub-01'\n",
    "num_runs = vdc_n_runs\n",
    "\n",
    "bold_data = load_data(directory, subject_name, mask_name='', num_runs=3, zscore_data=False).T\n",
    "labels = load_labels(directory, subject_name)\n",
    "\n",
    "blockwise_data, blockwise_labels = blockwise_sampling(bold_data, labels, events_per_block=10)\n",
    "\n",
    "print('shape - bold data:', np.shape(bold_data))\n",
    "print('shape - labels:',np.shape(labels))\n",
    "print('shape - blockwise_data:',np.shape(blockwise_data))\n",
    "print('shape - blockwise_labels:',np.shape(blockwise_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eventwise_data, eventwise_labels = bold_data, labels\n",
    "events_per_block=10\n",
    "\n",
    "# How many events are expected\n",
    "expected_blocks = int(eventwise_data.shape[0] / events_per_block)\n",
    "\n",
    "# Average the BOLD data for each block of trials into blockwise_data\n",
    "# Downsample labels into blockwise_labels\n",
    "n_events, n_voxels = np.shape(eventwise_data)\n",
    "boldwise_data = np.zeros((expected_blocks, n_voxels))\n",
    "for t in range(expected_blocks):\n",
    "    t_start, t_end = t * events_per_block, (t + 1) * events_per_block\n",
    "    boldwise_data[t,:] = np.mean(eventwise_data[t_start:t_end, :], axis = 0)\n",
    "\n",
    "# Report the new variable sizes\n",
    "print('Expected blocks: %d; Resampled blocks: %d' % (expected_blocks, blockwise_data.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Self-study:** We introduce a simple kind of debugging here, as we print both the number of expected and resampled blocks. Thus, if something went wrong, we would be able to spot it the output. Learn about more ways of debugging your code, e.g. using [if statements](https://stackoverflow.com/questions/5142418/what-is-the-use-of-assert-in-python) and [assertion statements](https://wiki.python.org/moin/UsingAssertionsEffectively)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Covariance <a id=\"covariance\"></a>\n",
    "\n",
    "As a precursor to understanding dimensionality reduction techniques, we need to learn how to compute the covariance matrix because it is often used in these methods.  \n",
    "\n",
    "We are going to work with whole-brain data this time. You might have memory issues but this is an important problem to grapple with. There are nearly 1 million voxels in every volume we acquire, of which about 15% are in the brain. Despite >100,000 voxels, we only have <1000 time points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preset variables\n",
    "# Make sure you edit the following line to reflect the directory where you are accessing the VDC dataset\n",
    "# dir = '/gpfs/milgram/data/cmhn-s18/datasets/vdc/' #Yale\n",
    "# dir = '/jukebox/pniintel/brainiak_edu/datasets/vdc/' #Princeton\n",
    "# dir ='/home/NEU350/datasets/vdc/'\n",
    "print(vdc_data_dir)\n",
    "\n",
    "num_runs=3\n",
    "TR=1.5\n",
    "hrf_lag = 4.5  # In seconds what is the lag between a stimulus onset and the peak bold response\n",
    "shift_size = int(hrf_lag / TR)  # Convert the shift into TRs\n",
    "\n",
    "sub_id = 1\n",
    "\n",
    "# Convert the number into a participant folder name\n",
    "if (sub_id < 10):\n",
    "    sids = '0' + str(sub_id)\n",
    "else:\n",
    "    sids = str(sub_id)   \n",
    "\n",
    "# Specify the subject name\n",
    "sub = 'sub-' + sids\n",
    "\n",
    "# Load subject labels\n",
    "stim_label_allruns = load_labels(vdc_data_dir, sub)\n",
    "\n",
    "# Load the fMRI data\n",
    "epi_mask_data_all = load_data(vdc_data_dir, sub, mask_name='')\n",
    "\n",
    "# This can differ per participant\n",
    "print(sub, '= TRs: ', epi_mask_data_all.shape[1], '; Voxels: ', epi_mask_data_all.shape[0])\n",
    "TRs_run = int(epi_mask_data_all.shape[1] / num_runs)\n",
    "\n",
    "# Convert the timing into TR indexes\n",
    "stim_label_TR = label2TR(stim_label_allruns, num_runs, TR, TRs_run)\n",
    "\n",
    "# Shift the data some amount\n",
    "stim_label_TR_shifted = shift_timing(stim_label_TR, shift_size)\n",
    "\n",
    "# Perform the reshaping of the data\n",
    "bold_data, labels = reshape_data(stim_label_TR_shifted, epi_mask_data_all)\n",
    "\n",
    "# Down sample the data to be blockwise rather than trialwise\n",
    "bold_data, labels = blockwise_sampling(bold_data, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The covariance of two variables is calculated as follows: $$ Cov(X,Y) = \\frac{\\sum_{1}^{N}(X-\\bar{X})(Y-\\bar{Y})}{(N-1)}$$\n",
    "where $\\mbox{  }  \\bar{X} = mean(X), \\mbox{  } \\bar{Y} = mean(Y), \\mbox{  } N = \\mbox{number of samples } $\n",
    "\n",
    "In fMRI, X and Y could be time-series data for two voxels in the simplest case. We could extend this analysis to creating a covariance matrix for a set of voxels.\n",
    "\n",
    "**Exercise 2:** <a id=\"ex2\"></a> Compute the covariance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter your code here\n",
    "\n",
    "# Compute the mean of one column of the bold data X\n",
    "\n",
    "# Compute the mean of any other column of the bold data  Y\n",
    "\n",
    "# Compute the differences from the mean for X and Y.\n",
    "\n",
    "# Compute the summed product of the differences.\n",
    "\n",
    "# Compute the covariance\n",
    "\n",
    "\n",
    "# Compare your result to the answer got by using np.cov(X,Y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The covariance is dependent on the units of the measurement. Its value is thus not easily interpretable or comparable across datasets -- e.g. is there a strong relationship between X and Y if the covariance is 200 as compared to 2 or 2000?\n",
    "\n",
    "Correlation solves this problem by normalizing the range of the covariance from -1 to +1.\n",
    "\n",
    "$$ Corr(X,Y) = \\frac{Cov(X,Y)}{\\sqrt{\\frac{\\sum_{1}^{N}(X-\\bar{X})^2}{(N-1)}}\\sqrt{\\frac{\\sum_{1}^{N}(Y-\\bar{Y})^2}{(N-1)}}}$$\n",
    "\n",
    "**Exercise 3:** <a id=\"ex3\"></a> Compute the correlation manually and with a pre-specified function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the correlation manually\n",
    "\n",
    "# Now with a function  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 4**: <a id=\"ex4\"></a> np.cov can take a matrix as input and calculates the covariance matrix of all columns. Extend the **covariance** computation to a group of 100 voxels in order to make a voxel by voxel covariance matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert your code here.\n",
    "\n",
    "# Subselect 100 voxels from bold_data into a matrix.\n",
    "\n",
    "# Use np.cov() to comput the covariance of this matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bonus:**  What is the maximum number of columns/voxels in the present dataset from which you could calculate the covariance matrix without exceeding available memory (currently 12GB)? Solve this analytically rather than by trial and error.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. PCA <a id=\"pca\"></a>\n",
    "\n",
    "We will use PCA (principal component analysis) to **reduce the dimensionality** of the data. Some voxels may contain correlated information or no information and so the original voxel-dimensional data can be projected into a lower-dimensional \"component\" space without losing much information.\n",
    "\n",
    "![image](https://cdn-images-1.medium.com/max/1200/1*Iri_LDMXuz2Qac-8KPeESA.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We now use the PCA function in scikit-learn to reduce the dimensionality of the data\n",
    "# The number of components was chosen arbitrarily and is likely too high\n",
    "pca = PCA(n_components=20)\n",
    "bold_pca = pca.fit_transform(bold_data)\n",
    "\n",
    "print(bold_data.shape)\n",
    "print(bold_pca.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Plot the PCA dataset for one subject <a id=\"plot_pca\"></a>\n",
    "\n",
    "Let's visualize the variance in the data along different component dimensions. Dimension 1 must display the largest variation and the later dimensions should start clumping the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting plotting parameter\n",
    "b=75\n",
    "\n",
    "# Plot\n",
    "n_plots = 4\n",
    "components_to_plot = [0,1,2,19]\n",
    "\n",
    "f, axes = plt.subplots(1, n_plots, figsize=(14, 14/n_plots))\n",
    "\n",
    "for i in range(n_plots): \n",
    "    axes[i].hist(bold_pca[:, components_to_plot[i]], bins=b)\n",
    "    # mark the plots \n",
    "    axes[i].set_title('PC Dimension %d'%(components_to_plot[i]+1))\n",
    "    axes[i].set_ylabel('Frequency')\n",
    "    axes[i].set_xlabel('Value')    \n",
    "    axes[i].set_xticks([])\n",
    "    axes[i].set_yticks([])    \n",
    "\n",
    "f.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the relationship between variances across pairs of components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting plotting parameters\n",
    "alpha_val = .7\n",
    "\n",
    "# Plot\n",
    "n_plots = 3 \n",
    "f, axes = plt.subplots(1, n_plots, figsize=(14,5))\n",
    "\n",
    "axes[0].scatter(bold_pca[:,0], bold_pca[:,1], alpha=alpha_val, marker='.')\n",
    "axes[0].set_title('PCA Dimensions\\n1 x 2')\n",
    "\n",
    "axes[1].scatter(bold_pca[:,0],bold_pca[:,2], alpha=alpha_val, marker='.') \n",
    "axes[1].set_title('PCA Dimensions\\n1 x 3')\n",
    "\n",
    "axes[2].scatter(bold_pca[:,18],bold_pca[:,19], alpha=alpha_val, marker='.') \n",
    "axes[2].set_title('PCA Dimensions\\n18 x 19')\n",
    "\n",
    "for i in range(n_plots): \n",
    "    axes[i].axis('equal')\n",
    "    axes[i].set_xticks([])\n",
    "    axes[i].set_yticks([])    \n",
    "f.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Self-study:** Why are there 3 groups of data in the first two PCA components? Hint: they are not the three conditions!\n",
    "\n",
    "**Exercise 5:** <a id=\"ex5\"></a> Look up how to display the amount of variance in the original data explained by each component, called a [\"scree\" plot](https://www.theanalysisfactor.com/factor-analysis-how-many-factors/), [e.g. in this guide](https://www.analyticsvidhya.com/blog/2016/03/practical-guide-principal-component-analysis-python/). Make a scree plot based on the components above. Interpret the plot: How many components would be sufficient to account for most of the variance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare whole-brain classification accuracy without and with PCA dimensionality reduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the baseline, whole brain decoding accuracy.\n",
    "print('Original size: ', bold_data.shape)\n",
    "classifier = SVC(kernel=\"linear\", C=0.0001)\n",
    "\n",
    "start = time()\n",
    "score_all = classification(classifier, bold_data, labels)\n",
    "end = time()\n",
    "print('Accuracy: %0.2f; Run time: %0.2fs' %(score_all, end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the classifier on the PCA data with different numbers of components included.\n",
    "print('New size after PCA: ', bold_pca.shape)\n",
    "\n",
    "start = time()\n",
    "score_pca = classification(classifier, bold_pca, labels)\n",
    "end = time()\n",
    "print('Accuracy: %0.2f; Run time: %0.2fs' %(score_pca, end - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 6:** <a id=\"ex6\"></a> How does accuracy change when we change the number of components?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. ICA <a id=\"ica\"></a>\n",
    "\n",
    "PCA is not always the best choice to identify the most explanatory dimensions in our data. If our data is non-Gaussian, Independent Components Analysis (ICA) may be better for dimensionality reduction. Learn more about when to use ICA instead of PCA and how they differ using the [lecture slides form the Center of Aerospace Structures at the University of Colorado Boulder](https://www.colorado.edu/engineering/CAS/courses.d/ASEN6519.d/Lectures.d/Lecture10_11.6519.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute ICA\n",
    "ica = FastICA(n_components=3, whiten=1)  # Set parameters\n",
    "bold_ica = ica.fit_transform(np.transpose(bold_data))  # Reconstruct signals/sources\n",
    "mixing_matrix = ica.mixing_  # Get estimated mixing matrix\n",
    "\n",
    "print(bold_ica.shape)\n",
    "print(mixing_matrix.shape)  # (n_features, n_components)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Plot the ICA dataset for one subject <a id=\"plot_ica\"></a>\n",
    "\n",
    "Let's visualize the ICA sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the ICA sources.\n",
    "\n",
    "# Setting plotting parameters\n",
    "a=0.2\n",
    "size=20\n",
    "\n",
    "# Plot\n",
    "plt.subplots(1,3, figsize=(14,14/3))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.title('ICA Source\\n1 vs 2')\n",
    "plt.scatter(bold_ica[:,0], bold_ica[:,1], color='red', alpha=a,  marker='x', s=size)\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.title('ICA Source\\n1 vs 3')\n",
    "plt.scatter(bold_ica[:,0],bold_ica[:,2], color='green', alpha=a, marker='d', s=size)\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.title('ICA Source\\n2 vs 3')\n",
    "plt.scatter(bold_ica[:,1],bold_ica[:,2], color='blue', alpha=a, marker=\"*\", s=size)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the ICA weights\n",
    "\n",
    "x=list(range(len(mixing_matrix[:,0])))\n",
    "plt.gca().set_color_cycle(['red', 'green', 'blue'])\n",
    "plt.figure()\n",
    "plt.title('ICA Weights')\n",
    "plt.plot(x, mixing_matrix[:,0], x, mixing_matrix[:,1], x, mixing_matrix[:,2], linewidth=2)\n",
    "plt.legend(['Source 1', 'Source 2', 'Source 3'], loc='center left')\n",
    "plt.xlabel('Time (Block)')\n",
    "plt.ylabel('Weight')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 7:** <a id=\"ex7\"></a> In the above analyses, three sources were modelled. What do you think these sources represent? If you ran this analysis with more than 3 sources, what do you think those additional sources would represent?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 8:** <a id=\"ex8\"></a> Redo the analysis with data created with a PPA mask and plot the results. Are there any differences between the ICA analyses using an FFA mask and the analyses using a PPA mask?."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Dimensionality reduction with feature selection <a id=\"dim_red\"></a>\n",
    "\n",
    "You have used dimensionality reduction techniques to speed up and possibly improve classification. The choice of parameters for the classifiers has been arbitary and the number of dimensions for PCA and ICA was based on your observation. Below we'll create pipeline to optimize these parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Feature Selection: ROI <a id=\"roi\"></a>\n",
    "\n",
    "In previous classes you have been doing voxel selection, even if you didn't know. Using the FFA, PPA, or any other mask is an example of voxel selection that can greatly improve your decoding accuracy if you have reason to believe that discriminative information is contained in the ROI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the classifier on the FFA masked data with blockwise sampling\n",
    "epi_mask_data_FFA = load_data(vdc_data_dir, sub, mask_name='FFA')\n",
    "bold_FFA, labels = reshape_data(stim_label_TR_shifted, epi_mask_data_FFA)\n",
    "bold_FFA, labels = blockwise_sampling(bold_FFA, labels)\n",
    "\n",
    "start = time()\n",
    "score_FFA = classification(classifier, bold_FFA, labels)\n",
    "end = time()\n",
    "print('Accuracy: %0.2f; Run time: %0.2fs' %(score_FFA, end - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Self-study:** The FFA and PPA were defined using a functional localizer in this dataset. Do you think this presents any concerns?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 5.2 Feature Selection: Top n Values <a id=\"top_n\"></a>\n",
    "\n",
    "We have as many features as we have voxels. We can reduce the feature space using metrics of our choice and picking the top n voxels that satisfy a metric. Below, we first standardize the data and take the top n voxels that had the highest mean activation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the 95th percentile for the mean voxel activation across time/blocks to use as a cut-off value\n",
    "mean_threshold = np.percentile(np.mean(bold_data, axis=0), 95)\n",
    "\n",
    "# Use the cut-off value to access all voxels with top 5% mean activity\n",
    "bold_top_N = bold_data[:, mean_threshold <= np.mean(bold_data, axis=0)]\n",
    "\n",
    "print('Top_N size: ', bold_top_N.shape)\n",
    "\n",
    "score_top_N = classification(classifier, bold_top_N, labels)\n",
    "\n",
    "print('Accuracy: %0.2f' %(score_top_N))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Self-study:** An alternative way to calculate the percentile is to use [SelectPercentile from sklearn.feature_selection](http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectPercentile.html). Replicate the above dimensionality reduction procedure using the SelectPercentile method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Feature Selection: Variance Threshold <a id=\"var_thresh\"></a>\n",
    "\n",
    "There are also functions built into sci-kit learn that we could use. One measure that we can use to select voxels is variance, as it can be useful to detect and reomve features with relatively low variance. We use the [sklearn VarianceThreshold method](http://scikit-learn.org/stable/modules/feature_selection.html#variance-threshold)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take the 95th percentile of variance\n",
    "var_threshold = np.percentile(np.std(bold_data, axis=0) ** 2, 95)\n",
    "\n",
    "selector = VarianceThreshold(threshold=var_threshold)\n",
    "bold_VT = selector.fit_transform(bold_data)\n",
    "\n",
    "print('Top variance size: ', bold_VT.shape)\n",
    "\n",
    "score_VT = classification(classifier, bold_VT, labels)\n",
    "\n",
    "print('Accuracy: %0.2f' %(score_VT))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Feature Selection: Univariate <a id=\"univariate\"></a>\n",
    "\n",
    "We can also use a variety of univariate methods to do feature selection using methods built into scikit-learn. We use  [chi-squared values](http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.chi2.html#sklearn.feature_selection.chi2) and the [SelectKBest method](http://scikit-learn.org/stable/modules/feature_selection.html#univariate-feature-selection) below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bold_chi = SelectKBest(chi2, k=100).fit_transform(bold_data, labels)\n",
    "print('K top features: ', bold_chi.shape)\n",
    "\n",
    "score_chi = classification(classifier, bold_chi, labels)\n",
    "print('Accuracy: %0.2f' %(score_chi))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Self-Study:** Not sure what is being tested here? Go find out!\n",
    "\n",
    "**Exercise 9:**<a id=\"ex9\"></a> Apply a dimensionality reduction procedure other than those above that improves performance. What is its run time?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5 Feature Selection: Recursive Feature Elimination (RFE) <a id=\"rfe\"></a>\n",
    "\n",
    "This technique eliminates features iteratively based on a measure that we can define, e.g., classification accuracy. At each step of the iteration a certain number of features are eliminated. This is done iteratively until an optimal set of features are found. Importantly, cross-validation is used during each iteration to select the best set of features at each stage. scikit-learn makes this incredibly easy with the [method RFECV](http://scikit-learn.org/stable/modules/feature_selection.html#recursive-feature-elimination).\n",
    "\n",
    "Because RFE is running many iterations of your classifier, you want to be careful that your data does not have too many features or else it will run for a long time. Also, the number of iterations is   dependent on both the number of features in your data and on how you set the step size of the RFE method. The step size corresponds to the number of features to remove at each iteration. Thus, the number of iterations will roughly be nb_of_features/step_size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The \"accuracy\" scoring is proportional to the number of correct classifications\n",
    "rfecv = RFECV(estimator=classifier, \n",
    "              step=500, \n",
    "              cv=StratifiedShuffleSplit(n_splits=3, test_size=0.1),\n",
    "              scoring='accuracy')\n",
    "rfecv.fit(bold_VT, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 10**: <a id=\"ex10\"></a> Report and visualize the results of RFE, including the optimal number of features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Novel contribution:** <a id=\"novel\"></a> be creative and make one new discovery by adding an analysis, visualization, or optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contributions <a id=\"contributions\"></a>\n",
    "\n",
    "M. Kumar, C. Ellis and N. Turk-Browne produced the initial notebook  \n",
    "T. Meissner minor edits and added the ICA section  \n",
    "Q. Lu revise PCA plots "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
