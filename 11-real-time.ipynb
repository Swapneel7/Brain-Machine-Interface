{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real-Time fMRI Analysis\n",
    "**V.0.1 - Alpha testing, [Contributions](#contributions)** \n",
    "\n",
    "Typically when we run fMRI experiments we create a sequence of stimuli or events we want to show the participant and then observe their brain activity in response to these pre-specified events. This allows for an assessment of the correlation between tasks and BOLD activity. Thus the causal interpretability of fMRI is limited. Real-time is a step in the direction of drawing causal inferences with fMRI. Specifically, rather than just being a dependent variable, brain activity becomes part of the design as a kind of independent variable. For more on real-time fMRI, see e.g. [Caria et al. (2011, The Neuroscientist)](https://doi.org/10.1177/1073858411407205).\n",
    "\n",
    "Take for instance a real-time experiment like [deBettencourt et. al., (2015, Nature Neuroscience)](http://ntblab.yale.edu/wp-content/uploads/2015/03/deBettencourt_NN_2015.pdf). In this case the brain activity was used to make the task easier or harder depending on the participant's attentional state, with the goal of training them to attend better. However, if the brain activity was too noisy or analyzed inappropriately, read out from brain regions that didn't contain information about attentional state, or taken from another participant's brain (as in the control condition), then the participant should not improve.\n",
    "\n",
    "There are no second chances with real-time fMRI because your analysis is part of data collection. For example, you can't decide to use a different preprocessing step, classifier algorithm, parameter setting, etc. after the fact. This is why preparation, piloting, and efficient code is critical for real time.\n",
    "\n",
    "\n",
    "## Goal of this script\n",
    "    1. Learn to design a real-time fMRI experiment  \n",
    "    2. Run a real-time fMRI analysis using simulated data  \n",
    "\n",
    "\n",
    "## Table of Contents\n",
    "[1. The Real-Time Workflow](#rt_wf)  \n",
    ">[1.1 Data File Preparation](#data_prep)  \n",
    ">[1.2 File Watcher](#file-watch)   \n",
    ">[1.3 Preprocess TR](#preprocess-tr)  \n",
    ">[1.4 Training A Model](#real-time-train)  \n",
    ">[1.5 Classifying New Volumes](#real-time-test)  \n",
    ">[1.6 Modify Stimulus After Classification](#mod-stim)  \n",
    "  \n",
    "[2. Running a Real-Time Simulation](#real-time-sim)\n",
    "\n",
    "[3. Adaptive Real-Time Experiments](#real-time-change)\n",
    "\n",
    "\n",
    "Exercises\n",
    "1. [Exercise 1](#ex1)  \n",
    "2. [Exercise 2](#ex2)  \n",
    "3. [Exercise 3](#ex3)  \n",
    "4. [Exercise 4](#ex4)  \n",
    "5. [Exercise 5](#ex5)  \n",
    "6. [Exercise 6](#ex6)  \n",
    "7. [Exercise 7](#ex7)  \n",
    "8. [Exercise 8](#ex8)   \n",
    "[Novel contribution](#novel)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. The Real-Time Workflow <a id=\"rt_wf\"></a>\n",
    "\n",
    "The following sequence of steps are necessary for successfully running a real-time fMRI Analysis. \n",
    "\n",
    "1. [Data File Preparation](#data_prep): Setup a folder where fMRI volumes will be stored as they are created.\n",
    "        \n",
    "2. [File Watcher](#file-watch): A function that looks for a volume to process.\n",
    "\n",
    "3. [Preprocess TR](#preprocess): Preprocess the TR to prepare it for classification\n",
    "\n",
    "4. [Training A Model](#real-time-train): Take the data you have set aside for training and create your classifier model.\n",
    "\n",
    "4. [Classifying New Volumes](#real-time-test): Take an epoch of data and classify that, assigning it a label.\n",
    "\n",
    "6. [Modify Stimulus After Classification](#mod-stim) The classifier results influence the next stimulus shown to the participant.\n",
    "\n",
    "Here is one example pipeline:\n",
    "\n",
    "![alt text](https://media.nature.com/m685/nature-assets/neuro/journal/v18/n3/images/nn.3940-F1.jpg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np  # type: ignore\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.linear_model import LogisticRegression  # type: ignore\n",
    "from watchdog.events import PatternMatchingEventHandler  # type: ignore\n",
    "from watchdog.observers import Observer  # type: ignore\n",
    "from queue import Queue\n",
    "from sklearn import svm\n",
    "from sklearn import linear_model\n",
    "import scipy.stats\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Data file preparation <a id=\"data_prep\"></a>\n",
    "\n",
    "As you know, an fMRI data volume is generated for each TR. The scanner outputs these files one-by-one in DICOM format. You will first need to read the DICOM data into a numpy array. The code below will then process the numpy array. One way to read DICOM files into numpy is by using the package [dicom-numpy](http://dicom-numpy.readthedocs.io/en/latest/).\n",
    "\n",
    "For this notebook, we will generate simulated data using the BrainIAK [fmrism module](http://brainiak.org/docs/brainiak.utils.html?highlight=fmrisim#module-brainiak.utils.fmrisim). The function *generate_data.py* simulates a two condition experiment where each condition is blocked for 10s with no ISI between events. More specifically, this function simulates fMRI noise and then inserts signal in two regions corresponding to the two conditions with the appropriate HRF. \n",
    "\n",
    "**Self-Study:** Investigate the *generate_data.py* function to learn how it is working and what brain activity distinguishes between conditions.\n",
    "\n",
    "**Exercise 1:**<a id=\"ex1\"></a> How is *generate_data.py* deciding the order of the two conditions? Is this how you would design an fMRI study? If not, modify the script to what you think is a better design."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**A:**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To simulate real time data, this function outputs a TR every 2 s and saves it as a numpy matrix in the *fmrisim/data/* folder. To run the script, run the cell below.\n",
    "\n",
    "*Note that if you run the script on milgram but do not participate in the course, you will run into this error at some point: \\\"sbatch: error: Batch job submission failed: Invalid account or account/partition combination specified\\\". This happens because the .sh-scripts are set up to run on a dedicated partition for the course. To solve the error, change the .sh-script file (e.g. using the nano command) and change the line #SBATCH --partition cmhn-s18 to #SBATCH --partition short and delete the line #SBATCH -A cmhn-s18 before running the scripts.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the generate_data function\n",
    "!sbatch ./11-real-time/run_generate_data.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any time you want to simulate the acquisition of data you should delete the contents of this folder (`rm 11-real-time/fmrisim/data/*`) and then run it again.\n",
    "\n",
    "Below we will set the paths to be used. We also want to specify the proportion of trials that will be used for training the model (time until the real-time neurofeedback kicks in)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_dir = './11-real-time/fmrisim/'  # Where is information stored that is needed for generating data\n",
    "data_dir = input_dir + 'data/'\n",
    "file_pattern = 'rt_{0:0>3}.npy'  # What is the pattern of file that wil be created\n",
    "\n",
    "train_count = 40  # Number of trials that will be used for training. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 File watcher <a id=\"file-watch\"></a>\n",
    "\n",
    "For real time, we want to monitor as every TR comes in. The simple way to do this is to just set up a loop which checks whether the file exists and waits until it does, the loop below does this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a file watching algorithm\n",
    "def tr_watcher_simple(filename,\n",
    "                      sleep_time=0.1,  # temporal resolution of the file watcher\n",
    "                     ):\n",
    "    \n",
    "    # While the file doesn't exist, loop and wait\n",
    "    while not os.path.exists(filename):\n",
    "        time.sleep(sleep_time)  # How long do you want to wait for\n",
    "        \n",
    "    # When the file exists, load it and output it\n",
    "    vol = np.load(filename)\n",
    "    \n",
    "    return vol"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, this procedure is prone to error and is inefficient. For instance, if a volume is added just after a check, this code waits the sleep window before checking again. Moreover, this procedure can crash since the file names often exist before the file contents are created (and so if they are read in too soon then the code may crash).\n",
    "\n",
    "Instead, to process volumes as soon as they are finished being created, a file watcher function continuously polls for new files. Once a new file is found, it is added to a queue and triggers a call to the next processing step. The [watchdog package](https://pythonhosted.org/watchdog/) is used for ths purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a file watching algorithm\n",
    "def tr_watcher(filename, file_queue):\n",
    "    \n",
    "    # Does the file exist?\n",
    "    file_exists = os.path.exists(filename)\n",
    "    \n",
    "    # While the file doesn't exist, loop and wait\n",
    "    while not file_exists:\n",
    "        \n",
    "        # look for file creation event\n",
    "        event = file_queue.get()\n",
    "        \n",
    "        # If there is an event then save it\n",
    "        if event.src_path == filename:\n",
    "            file_exists = True\n",
    "    \n",
    "    # When the file exists, load it and output it\n",
    "    vol = np.load(filename)\n",
    "    \n",
    "    return vol\n",
    "\n",
    "\n",
    "# Create a class of events for \n",
    "class file_notify_handler(PatternMatchingEventHandler):\n",
    "\n",
    "    # Initialize the object being created\n",
    "    def __init__(self, queue, file_pattern):\n",
    "        super().__init__(patterns=file_pattern)\n",
    "        self.q = queue\n",
    "    \n",
    "    # When an event occurs, put it in the queue\n",
    "    def on_created(self, event):\n",
    "        self.q.put(event)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to check out this file watcher in action. Remember to clear your `11-real-time/fmrisim/data` directory and re-launch *run_generate_data.sh*. This script will first play catch up and then print every time a new TR comes in until the training set is acquired."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_observer = Observer()\n",
    "file_queue = Queue()  # type: ignore\n",
    "\n",
    "# set up the notifications for when a new TR is created\n",
    "notify_file_pattern = '*.npy'  \n",
    "file_notify = file_notify_handler(file_queue, [notify_file_pattern])  \n",
    "file_observer.schedule(file_notify, data_dir, recursive=False)  \n",
    "file_observer.start()  \n",
    "\n",
    "for idx in range(train_count): \n",
    "\n",
    "    # What file name are you going to load\n",
    "    next_filename = data_dir + file_pattern.format(idx) \n",
    "    vol = tr_watcher(next_filename, file_queue)\n",
    "\n",
    "    # When the file exists, load it and output it\n",
    "    print('Recieved:', next_filename)\n",
    "    \n",
    "file_observer.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Preprocess TR <a id=\"preprocess-tr\"></a>\n",
    "\n",
    "As each volume is received, it is necessary to preprocess it. You will have to make design choices on how to accomplish all preprocessing in an efficient manner so as to process incoming volumes. This preprocessing could involve numerous steps, including motion correction, masking and normalization. The simulator does not generate any motion artifacts so that step is unnecessary. Masking is critical because we don't want to feed the model irrelevant features. Normalization in space (as we will do) is easy because each time point can be treated independently; however, normalizing across time is hard since each additional TR will change the mean and SD, and thus the z-values of all preceding TRs. This is a more general consequence of real-time, wherein you know the past but not the future. This also impacts temporal filtering, which must be done with \"causal\" filters. For this reason, procedures for normalizing and filtering over time are still being developed/refined.\n",
    "\n",
    "**Self-study:** Look into normalizing voxels across time. Helpful information is found in the [StandardScaler](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) function of sklearn. For temporal filtering, compare the [filtfilt](https://docs.scipy.org/doc/scipy-0.18.1/reference/generated/scipy.signal.filtfilt.html) and [lfilter](https://docs.scipy.org/doc/scipy-0.18.1/reference/generated/scipy.signal.lfilter.html) functions in scipy.\n",
    "\n",
    "**Exercise 2:** <a id=\"ex2\"></a>  Create a preprocessing function. This function will take in a volume and a mask, perform masking and then z-score the masked voxels in space, and then output a 1-dimensional vector of preprocessed voxels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert code here\n",
    "def preprocess_vol(vol,\n",
    "                  mask):\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 3:** <a id=\"ex3\"></a> Do a speed test of the tr watcher and preprocessing functions to make sure they fit within 1 TR. Specifically add to the loop below appropriate time stamping and print commands. This should take less than 5 ms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert code here\n",
    "    \n",
    "# load mask\n",
    "mask_file = input_dir +'mask.npy'\n",
    "mask = np.load(mask_file)\n",
    "\n",
    "# allocate a matrix to hold the TR data, preset to NaNs. Each row is a TR vector\n",
    "tr_data = np.full((train_count, mask.sum()), np.nan)\n",
    "\n",
    "# set up the notifications for when a new TR is created\n",
    "file_observer = Observer()\n",
    "file_queue = Queue()  # type: ignore\n",
    "\n",
    "notify_file_pattern = '*.npy'  \n",
    "file_notify = file_notify_handler(file_queue, [notify_file_pattern])  \n",
    "file_observer.schedule(file_notify, data_dir, recursive=False)  \n",
    "file_observer.start()  \n",
    "\n",
    "# Cycle through TRs\n",
    "for idx in range(train_count): \n",
    "    \n",
    "    # Start the timer\n",
    "    \n",
    "    # What file name are you going to load\n",
    "    next_filename = data_dir + file_pattern.format(idx) \n",
    "    vol = tr_watcher(next_filename, file_queue)\n",
    "    \n",
    "    # Store the volume as a preprocessed vector\n",
    "    tr_data[idx, :] = preprocess_vol(vol, mask)\n",
    "    \n",
    "    # End the timer\n",
    "    \n",
    "    # Print the timing\n",
    "\n",
    "file_observer.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Training a model  <a id=\"real-time-train\"></a>\n",
    "\n",
    "After we have collected enough TRs we then train our classifier. [Like deBettencourt et al. (2015)](http://ntblab.yale.edu/wp-content/uploads/2015/03/deBettencourt_NN_2015.pdf) we will use an L2 logistic regression. Below a function is created to take in TR by voxel training data and a list of labels for each TR and then fit a regression function to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Train 2 Logistic Regression models\n",
    "def train_logistic(training_data,\n",
    "                   training_labels,\n",
    "                   parameters='l2'):\n",
    "    \n",
    "    # Train the model predicting state 2 (so that true means 1 and false means 0)\n",
    "    clf = LogisticRegression(penalty=parameters)\n",
    "    clf.fit(training_data, training_labels == 2)\n",
    "\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A critical step when training the model is shifting the labels. We expect that a stimulus will evoke activity 4-6 s after it's onset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the labels\n",
    "label_file = data_dir + 'labels.npy'\n",
    "labels = np.load(label_file)\n",
    "\n",
    "# How much do you need to shift the labels by in terms of TRs\n",
    "tr_shift = 3\n",
    "\n",
    "training_data   = tr_data[tr_shift:train_count, :]\n",
    "training_labels = labels[0:train_count - tr_shift]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model\n",
    "clf = train_logistic(training_data, training_labels[:,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now visualize this training in order to see what the model has learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefs = np.zeros(mask.shape)\n",
    "coefs[mask==1]=clf.coef_[0,:]\n",
    "\n",
    "plt.imshow(coefs[:,:,16])\n",
    "plt.title('Classifier coeffients for slice through the brain')\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Classifying new volumes  <a id=\"real-time-test\"></a>\n",
    "\n",
    "Now that we have trained a model, we want to classify incoming information according to that model. Since most classifiers come with the *.fit* and *.predict* formulation, this is very easy to do (after some reshaping). In the case of a logistic regression, the outcome is simply `True` or `False`. In this case, `True` means condition 2 and `False` means condition 1. We can then do that for every new TR as it comes in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a new TR\n",
    "next_filename = data_dir + file_pattern.format(train_count + 1)\n",
    "vol = tr_watcher(next_filename, file_queue)\n",
    "\n",
    "# Store the volume as a preprocessed vector\n",
    "new_data = preprocess_vol(vol, mask)\n",
    "prediction = clf.predict(new_data.reshape(1, -1))\n",
    "\n",
    "print('Prediction for TR %d is condition %d and the label is %d' % (train_count + 1, prediction + 1, labels[idx - tr_shift]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6 Modifying the stimulus after classification  <a id=\"mod-stim\"></a>\n",
    "\n",
    "The final step in the real-time analysis is to use the classifier results and provide feedback to the subject in the scanner. This could involve changing the composition of the stimulus, changing the task difficulty or providing feedback directly. Although we won't do this here, it can be easily implemented in any experiment code from [Psychtoolbox](http://psychtoolbox.org/) or [PsychoPy](http://www.psychopy.org/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Running a Real-Time Simulation  <a id=\"real-time-sim\"></a>\n",
    "\n",
    "Now that we have all the parts of the real-time workflow, we can put them together to run a simulated real-time experiment. The function below named *realtime* does this. \n",
    "\n",
    "This takes as an input the `fmrisim` directory. It also takes the number of TRs that are used for training and a function which specifies how to run the classifier (e.g. *train_logistic*). The final input *incremental_batch* will be discussed later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def realtime(input_dir,\n",
    "             train_count,\n",
    "             clf_obj=train_logistic,\n",
    "             incremental_batch=0,\n",
    "            ):\n",
    "    \n",
    "    # Where is the data?\n",
    "    data_dir = input_dir + 'data/'  \n",
    "        \n",
    "    \n",
    "    # While the file doesn't exist, loop and wait\n",
    "    label_file = data_dir + 'labels.npy'\n",
    "    while not os.path.exists(label_file):\n",
    "        time.sleep(0.1)  # How long do you want to wait for\n",
    "    \n",
    "    # load labels\n",
    "    labels = np.load(label_file)\n",
    "\n",
    "    # How much do you need to shift the labels by in terms of TRs\n",
    "    tr_shift = 3\n",
    "    \n",
    "    # load mask\n",
    "    mask_file = input_dir +'mask.npy'\n",
    "    mask = np.load(mask_file)\n",
    "\n",
    "    # allocate a matrix to hold the TR data, preset to NaNs. Each row is a TR vector\n",
    "    tr_data = np.full((len(labels), mask.sum()), np.nan)\n",
    "\n",
    "    # Set up the figure\n",
    "    plt.figure()  # Set up figure\n",
    "    plt.plot((train_count, train_count), (0, 3), 'g')\n",
    "    plt.xlim((0, len(labels)))\n",
    "    plt.ylim((0, 3))\n",
    "    plt.title('Searching for the first TR')\n",
    "    is_print=0\n",
    "    \n",
    "    # set up the notifications for when a new TR is created\n",
    "    file_observer = Observer()\n",
    "    file_queue = Queue()  # type: ignore\n",
    "    \n",
    "    file_notify = file_notify_handler(file_queue, [notify_file_pattern])  \n",
    "    file_observer.schedule(file_notify, data_dir, recursive=False)  \n",
    "    file_observer.start()  \n",
    "\n",
    "    # Listen for TRs\n",
    "    num_correct = 0  # Preset the number of correct answers to zero\n",
    "    for idx in range(len(labels)): \n",
    "\n",
    "        # What file name are you going to load\n",
    "        next_filename = data_dir + file_pattern.format(idx, '02d')  \n",
    "        vol = tr_watcher(next_filename, file_queue)\n",
    "\n",
    "        # Store the volume as a preprocessed vector\n",
    "        tr_data[idx, :] = preprocess_vol(vol, mask)\n",
    "\n",
    "        plt.plot(range(idx), labels[0:idx], 'r-')\n",
    "\n",
    "        # Collect TRs for training\n",
    "        if idx < train_count:\n",
    "            plt.title('TR: %d for training' % idx)\n",
    "        elif idx == train_count: # Is this time to train the classifier\n",
    "\n",
    "            # Train the classifier\n",
    "            trainStart = time.time()\n",
    "            plt.title(\"Sufficient TRs collected, training the model\")\n",
    "\n",
    "            # Train the classifier\n",
    "            clf = clf_obj(tr_data[tr_shift:train_count, :], labels[0:train_count - tr_shift][:,0])\n",
    "\n",
    "            # Report the training duration\n",
    "            print(\"Completed training in %0.2f sec\" % (time.time() - trainStart))\n",
    "\n",
    "        elif idx > train_count:  # Is this after training (is it testing)\n",
    "\n",
    "            # Pull out the predictions of the model for this TR\n",
    "            prediction = clf.predict(tr_data[idx, :].reshape(1, -1))\n",
    "\n",
    "            # If it is a boolean (0 or 1) then add 1 to turn it into the labels\n",
    "            if prediction.dtype=='bool':\n",
    "                prediction = prediction + 1\n",
    "\n",
    "            if prediction == labels[idx - tr_shift]:\n",
    "                num_correct += 1\n",
    "\n",
    "            plt.scatter(idx, prediction)\n",
    "            accuracy = num_correct / (idx - train_count)\n",
    "            plt.title('TR: %d; Total accuracy: %0.2f' % (idx + 1, accuracy))\n",
    "\n",
    "            # Do you want to create a new batch for training if doing an incremental fit\n",
    "            if incremental_batch > 0 and np.mod((idx - train_count), incremental_batch) == 0:\n",
    "\n",
    "                # When does this batch start\n",
    "                start_idx = idx - incremental_batch\n",
    "\n",
    "                # Feed in the classifier to be updated with the current batch size\n",
    "                clf = clf_obj(tr_data[start_idx + tr_shift:idx, :], labels[start_idx:idx - tr_shift], clf)\n",
    "\n",
    "                # Mark where the new batch was loaded in\n",
    "                plt.plot((idx, idx), (0, 3), 'k')\n",
    "\n",
    "        # Plot the figure\n",
    "        display.clear_output(wait=True)\n",
    "        display.display(plt.gcf())\n",
    "        plt.xlim((0, len(labels)))\n",
    "        plt.ylim((0, 3))\n",
    "        \n",
    "    # Stop listening\n",
    "    file_observer.stop()\n",
    "    \n",
    "    # Return the accuracy overall\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    realtime(input_dir=input_dir,\n",
    "             train_count=train_count,\n",
    "             clf_obj=train_logistic,\n",
    "             incremental_batch=0,\n",
    "            )\n",
    "except Exception as err:\n",
    "    file_observer.stop()\n",
    "    print(\"Exception: {}\".format(err))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 4:**<a id=\"ex4\"></a> Re-run the *realtime* function using a different classifier. Create a new classifier function with a different kernel and run it below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Insert new classifier object\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the realtime function with this new classifier object\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 5:**<a id=\"ex5\"></a> Create a new copy of the *realtime* function below and rename it. Then edit it in order to plot the classifier decision evidence rather than the labels. Make sure that you have an appropriate classifier that outputs the decision evidence for each prediction (and re-scale your figure appropriately)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert code below\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Adaptive Real-Time Experiments  <a id=\"real-time-change\"></a>\n",
    "\n",
    "In many real-time experiments, the goal is to change brain function via neurofeedback. For instance, neural representations might change because the experiment trains participants to use different parts of their brain to process a stimulus. In such cases, a classifier trained at the start of the experiment will be wrong by the end of the experiment because the underlying feature space across voxels in the brain has changed. There are algorithms that can be used in these cases, which allow the model fit to be updated as new training examples come in. This can also be used to refine your model with more training data even if the underlying features are stable. \n",
    "\n",
    "This approach to dynamically training classifiers is called incremental or online learning and [sklearn](http://scikit-learn.org/stable/modules/scaling_strategies.html#incremental-learning) has a number of classifiers that allow this. Below we specify one classifier with this functionality. First we will run it without incremental updating as a baseline. \n",
    "\n",
    "Beware: The initialization of these functions can have a dramatic effect on performance, which you can evaluate by using a random seed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_incremental(training_data,\n",
    "                      training_labels,\n",
    "                      parameters=None,\n",
    "                     ):\n",
    "    \n",
    "    all_classes = np.array([1, 2])  # Need to say all the labels, in case you want to test out of sample\n",
    "    \n",
    "    # Get the clf\n",
    "    if parameters is None:\n",
    "        # Create the linear if it hasn't already been passed in by parameters\n",
    "        clf = linear_model.SGDClassifier()\n",
    "    else:\n",
    "        clf = parameters  # Pull out the classifier\n",
    "        \n",
    "    # Fit the training data (either initializing the clf or updating it)    \n",
    "    clf.partial_fit(training_data,\n",
    "                    training_labels,\n",
    "                    classes=all_classes,\n",
    "                   )\n",
    "    \n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    np.random.seed(0)\n",
    "    realtime(input_dir=input_dir,\n",
    "             train_count=train_count,\n",
    "             clf_obj=train_incremental,\n",
    "             incremental_batch=0,\n",
    "            )\n",
    "except Exception as err:\n",
    "    file_observer.stop()\n",
    "    print(\"Exception: {}\".format(err))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have run the classifier with static weights, we can try dynamically updating the classifier as we acquire more data. This is accomplished by telling realtime how often you want to update (here, after every additional 20 TRs). The *incremental_batch* variable specifies how many TRs are used in each batch of training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    np.random.seed(0)\n",
    "    realtime(input_dir=input_dir,\n",
    "                   train_count=train_count,\n",
    "                   clf_obj=train_incremental,\n",
    "                   incremental_batch=40,\n",
    "                  )\n",
    "except Exception as err:\n",
    "    file_observer.stop()\n",
    "    print(\"Exception: {}\".format(err))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 6:**<a id=\"ex6\"></a> Play around with the size of the batch that you will use to re-train the model. What are the advantages of using a small vs. a large batch? What batch size that you tried works best and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "So now that we have a classifier that can update its classifier weights, we can make a simulation of the data changing part way through the experiment. The *generate_data.py* script is currently set up to allow this with a few parameter changes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 7:**<a id=\"ex7\"></a> Generate a new set of data where the brain areas that discriminate between conditions switches halfway through the run (Hint: look at the parameters in *generate_data.py*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Insert code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 8:**<a id=\"ex8\"></a> Run a real-time analysis on this new dataset with and without incremental learning (using the optimal results from above) and compare the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Insert code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Self-Study** Take Exercise 8 to the next level by doing a parameter search over the different combinations of parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_values = range(10, 100, 10)\n",
    "train_values = range(10, 100, 10)\n",
    "results = np.zeros((len(batch_values), len(train_values)))\n",
    "for batch_counter in range(len(batch_values)):\n",
    "    for train_counter in range(len(train_values)):\n",
    "        \n",
    "        # Pull out the values\n",
    "        batch_size = batch_values[batch_counter]\n",
    "        train_size = train_values[train_counter]\n",
    "        \n",
    "        np.random.seed(0)\n",
    "        result = realtime(input_dir=input_dir,\n",
    "                          train_count=train_size,\n",
    "                          clf_obj=train_incremental,\n",
    "                          incremental_batch=batch_size,\n",
    "                         )\n",
    "\n",
    "        # Store the results\n",
    "        results[batch_counter, train_counter] =  result\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can visualize this parameter search by making a heat map\n",
    "plt.figure(figsize = (12,8))\n",
    "plt.imshow(results, interpolation = 'none')\n",
    "plt.xlabel(\"Incremental batch value\")\n",
    "plt.ylabel(\"# TRs used for training\")\n",
    "plt.title(\"Heatmap of SDG classification accuracy with different input parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Novel contribution:**<a id=\"novel\"></a> Be creative and make one new discovery by adding an analysis, visualization, or optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Contributions<a id=\"contributions\"></a>\n",
    "\n",
    "G. Wallace for providing initial code  \n",
    "M. Kumar, C. Ellis and N. Turk-Browne produced the initial notebook 4/11/18  "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
