{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "4988daf4-346a-4a15-b8f8-4802ccdfb87b"
    }
   },
   "source": [
    "# Full Correlation Matrix Analysis\n",
    "**V.0.1 - Alpha testing, [contributions](#contributions)**\n",
    "\n",
    "The connectivity analyses we performed previously examined the coupling of different regions of the brain. This coupling can capture information that activity-based univariate or multivariate techniques do not detect. For instance, imagine that voxel A and B are coupled during one task condition and voxel A and C are coupled during another task condition. In such a case, voxel A may always be active (and thus task differences are likely to be invisible to activity-based approaches), but its connectivity with other voxels will reveal task information.\n",
    "\n",
    "However, this creates a problem: to find these diagnostic voxels in an unbiased, data-driven fashion it is necessary to correlate every voxel in the brain with every other voxel. Simplifying assumptions can be made to reduce the complexity of this analysis. For instance, it is possible to downsample the voxels (e.g., using parcellation as in the previous session) in order to reduce the total number of correlations that are made. \n",
    "\n",
    "Fortunately, advances in machine learning, parallel computing and efficient coding have made it possible to calculate and analyze billions of correlations rapidly. These tools are part of the [BrainIAK toolbox](http://brainiak.org/) and are used for Full Correlation Matrix Analysis (FCMA). This method is outlined in detail in [Wang et al. (2015, Journal of Neuroscience Methods)](http://ntblab.yale.edu/wp-content/uploads/2015/06/Wang_JNM_2015.pdf). In what follows we will learn how to run FCMA and actually perform it on real data on a reasonable timescale.\n",
    "\n",
    "The logic of FCMA is as follows: take every voxel in the brain and correlate it with every other voxel in the brain for each trial and participant. Then, take each big correlation matrix and turn it into a vector so that every pairwise relationship is represented as a dimension in that vector. The vectors for each trial/condition can then be stacked so that we get an example by feature matrix. We can then feed that matrix into a classifier and determine whether the pattern of information across voxel pairs discriminates between conditions.  \n",
    "\n",
    "For this script we will use the face/scene dataset from [Wang et al. (2015)](https://doi.org/10.1016/j.jneumeth.2015.05.012), who in turn used localizer data from [Turk-Browne et al. (2012, JNeurosci)](https://doi.org/10.1523/JNEUROSCI.0942-12.2012).\n",
    "\n",
    "## Goal of this script\n",
    "1. Run FCMA feature selection\n",
    "2. Run FCMA classification\n",
    "3. Learn plotting tools for FCMA\n",
    "\n",
    "## Table of Contents\n",
    "[1. The FCMA Workflow](#fcma_wf)  \n",
    ">[1.1 Data Preparation](#data_prep)  \n",
    ">[1.2 Create an epoch file](#epoch)  \n",
    ">[1.3 Normalize data](#prep_for_fcma)  \n",
    ">[1.4 Compute correlations and voxel selection](#vox_sel)  \n",
    ">[1.5 Create voxel selection masks](#masks)  \n",
    ">[1.6 Classification](#classify)  \n",
    "\n",
    "[2. FCMA Batch Scripts](#fcma_batch)\n",
    ">[2.1 Inner loop (subsample)](#fcma_inner)  \n",
    ">[2.2 Outer loop (full sample)](#fcma_outer)  \n",
    ">[2.3 Permutation testing](#fcma_perm)  \n",
    "\n",
    "[3. Plotting the results](#plot) \n",
    ">[3.1 Plot the connectome](#connectome)  \n",
    ">[3.2 Plotting circos](#circos)  \n",
    "\n",
    "[4. MVPA and FCMA](#mvpa) \n",
    "\n",
    "### Exercises:  \n",
    ">[Exercise 1](#ex1)  \n",
    ">[Exercise 2](#ex2)    \n",
    ">[Exercise 3](#ex3)  \n",
    ">[Exercise 4](#ex4)  \n",
    ">[Exercise 5](#ex5)  \n",
    ">[Exercise 6](#ex6)  \n",
    ">[Exercise 7](#ex7)  \n",
    ">[Exercise 8](#ex8)  \n",
    ">[Exercise 9](#ex9)  \n",
    ">[Novel contribution](#novel) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 1. The FCMA Workflow <a id=\"fcma_wf\"></a>\n",
    "\n",
    "The following sequence of steps are necessary for successfully running FCMA using BrainIAK. \n",
    "\n",
    "1. [Data Preparation](#data_prep) \n",
    "\n",
    "2. [Create an epoch file](#epoch)\n",
    "    \n",
    "3. [Normalize data](#prep_for_fcma) \n",
    "\n",
    "4. [Correlation and voxel selection](#vox_sel)\n",
    "\n",
    "5. [Create voxel selection masks](#masks)\n",
    "\n",
    "6. [Classification](#classify)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "nbpresent": {
     "id": "1f7f9d75-833f-410f-8988-58c1618fa753"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "/opt/anaconda3/lib/python3.6/site-packages/nilearn/plotting/__init__.py:20: UserWarning: \n",
      "This call to matplotlib.use() has no effect because the backend has already\n",
      "been chosen; matplotlib.use() must be called *before* pylab, matplotlib.pyplot,\n",
      "or matplotlib.backends is imported for the first time.\n",
      "\n",
      "The backend was *originally* set to 'module://ipykernel.pylab.backend_inline' by the following code:\n",
      "  File \"/opt/anaconda3/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n",
      "    \"__main__\", mod_spec)\n",
      "  File \"/opt/anaconda3/lib/python3.6/runpy.py\", line 85, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/opt/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/opt/anaconda3/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n",
      "    app.start()\n",
      "  File \"/opt/anaconda3/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 478, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/opt/anaconda3/lib/python3.6/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\n",
      "    super(ZMQIOLoop, self).start()\n",
      "  File \"/opt/anaconda3/lib/python3.6/site-packages/tornado/ioloop.py\", line 888, in start\n",
      "    handler_func(fd_obj, events)\n",
      "  File \"/opt/anaconda3/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/opt/anaconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n",
      "    self._handle_recv()\n",
      "  File \"/opt/anaconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n",
      "    self._run_callback(callback, msg)\n",
      "  File \"/opt/anaconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n",
      "    callback(*args, **kwargs)\n",
      "  File \"/opt/anaconda3/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/opt/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n",
      "    return self.dispatch_shell(stream, msg)\n",
      "  File \"/opt/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n",
      "    handler(stream, idents, msg)\n",
      "  File \"/opt/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n",
      "    user_expressions, allow_stdin)\n",
      "  File \"/opt/anaconda3/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n",
      "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
      "  File \"/opt/anaconda3/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n",
      "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
      "  File \"/opt/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2728, in run_cell\n",
      "    interactivity=interactivity, compiler=compiler, result=result)\n",
      "  File \"/opt/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2850, in run_ast_nodes\n",
      "    if self.run_code(code, result):\n",
      "  File \"/opt/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2910, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-1-cf38df1ca447>\", line 4, in <module>\n",
      "    get_ipython().run_line_magic('matplotlib', 'inline')\n",
      "  File \"/opt/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2095, in run_line_magic\n",
      "    result = fn(*args,**kwargs)\n",
      "  File \"<decorator-gen-107>\", line 2, in matplotlib\n",
      "  File \"/opt/anaconda3/lib/python3.6/site-packages/IPython/core/magic.py\", line 187, in <lambda>\n",
      "    call = lambda f, *a, **k: f(*a, **k)\n",
      "  File \"/opt/anaconda3/lib/python3.6/site-packages/IPython/core/magics/pylab.py\", line 99, in matplotlib\n",
      "    gui, backend = self.shell.enable_matplotlib(args.gui)\n",
      "  File \"/opt/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2978, in enable_matplotlib\n",
      "    pt.activate_matplotlib(backend)\n",
      "  File \"/opt/anaconda3/lib/python3.6/site-packages/IPython/core/pylabtools.py\", line 308, in activate_matplotlib\n",
      "    matplotlib.pyplot.switch_backend(backend)\n",
      "  File \"/opt/anaconda3/lib/python3.6/site-packages/matplotlib/pyplot.py\", line 231, in switch_backend\n",
      "    matplotlib.use(newbackend, warn=False, force=True)\n",
      "  File \"/opt/anaconda3/lib/python3.6/site-packages/matplotlib/__init__.py\", line 1410, in use\n",
      "    reload(sys.modules['matplotlib.backends'])\n",
      "  File \"/opt/anaconda3/lib/python3.6/importlib/__init__.py\", line 166, in reload\n",
      "    _bootstrap._exec(spec, module)\n",
      "  File \"/opt/anaconda3/lib/python3.6/site-packages/matplotlib/backends/__init__.py\", line 16, in <module>\n",
      "    line for line in traceback.format_stack()\n",
      "\n",
      "\n",
      "  matplotlib.use('Agg')\n"
     ]
    }
   ],
   "source": [
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import time\n",
    "import sys\n",
    "import os\n",
    "\n",
    "from nilearn import plotting\n",
    "from nilearn.image import coord_transform\n",
    "\n",
    "import brainiak.utils.fmrisim as sim\n",
    "from brainiak.fcma.voxelselector import VoxelSelector\n",
    "from brainiak.fcma.preprocessing import prepare_fcma_data\n",
    "from brainiak.fcma.preprocessing import RandomType\n",
    "from brainiak.fcma.util import compute_correlation\n",
    "from brainiak import io"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Data Preparation <a id=\"data_prep\"></a>\n",
    "\n",
    "FCMA has tools to efficiently read in brain data but to do this, directories must be set up appropriately. Specifically, FCMA takes as an input a directory with the fMRI data you want to analyze. You can specify the suffix of the files you want to read in. Usually the suffix will just be '.nii.gz' but if you want to specify loading in only data from a certain condition then you might want a different kind of suffix. All data within the directory with the matching suffix will be loaded in and analyzed. \n",
    "\n",
    "Previous data we have analyzed were shared internally on Milgram. However, there is an increasing trend toward public data sharing and transparency (buzzword [Open Science](http://www.unesco.org/new/en/communication-and-information/portals-and-platforms/goap/open-science-movement/)), which may come into play for your final project... As an initial taste, we will use a public dataset for today's exercise. Although it is great that this data is available through the [BrainIAK fcma package](http://brainiak.org/docs/brainiak.fcma.html), it would be even better if it was stored in a a recognized repostiory like [NeuroVault](https://neurovault.org/), [OpenfMRI](https://openfmri.org/), or the [Open Science Framework](https://osf.io/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "100  1246    0  1246    0     0   1192      0 --:--:--  0:00:01 --:--:--     0\n",
      "100  251M  100  251M    0     0  38.8M      0  0:00:06  0:00:06 --:--:-- 51.2M\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0202101_conatt_localizer_std_bet.nii.gz\r\n",
      "0209101_conatt_localizer_std_bet.nii.gz\r\n",
      "0209102_conatt_localizer_std_bet.nii.gz\r\n",
      "0216101_conatt_localizer_std_bet.nii.gz\r\n",
      "0216102_conatt_localizer_std_bet.nii.gz\r\n",
      "0223101_conatt_localizer_std_bet.nii.gz\r\n",
      "0225101_conatt_localizer_std_bet.nii.gz\r\n",
      "0302101_conatt_localizer_std_bet.nii.gz\r\n",
      "0309101_conatt_localizer_std_bet.nii.gz\r\n",
      "0316101_conatt_localizer_std_bet.nii.gz\r\n",
      "0413101_conatt_localizer_std_bet.nii.gz\r\n",
      "0504101_conatt_localizer_std_bet.nii.gz\r\n",
      "0506101_conatt_localizer_std_bet.nii.gz\r\n",
      "0511101_conatt_localizer_std_bet.nii.gz\r\n",
      "0511102_conatt_localizer_std_bet.nii.gz\r\n",
      "0512101_conatt_localizer_std_bet.nii.gz\r\n",
      "0728101_conatt_localizer_std_bet.nii.gz\r\n",
      "0729101_conatt_localizer_std_bet.nii.gz\r\n",
      "fs_epoch_labels.npy\r\n",
      "mask.nii.gz\r\n",
      "prefrontal_top_mask.nii.gz\r\n",
      "readme.txt\r\n",
      "visual_top_mask.nii.gz\r\n"
     ]
    }
   ],
   "source": [
    "# Set paths\n",
    "data_dir = '/opt/public_FMRI/face_scene/'\n",
    "suffix = 'bet.nii.gz'\n",
    "mask_file =  data_dir + 'mask.nii.gz'\n",
    "epoch_file = data_dir + 'fs_epoch_labels.npy'\n",
    "\n",
    "# Print the directory to see its contents\n",
    "!ls $data_dir\n",
    "\n",
    "# Create an image object that can be used by FCMA to load in data efficiently.\n",
    "images = io.load_images_from_dir(data_dir, suffix)\n",
    "\n",
    "# Use a path to a mask file to create a binary mask for FCMA\n",
    "mask = io.load_boolean_mask(mask_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Create an epoch file <a id=\"epoch\"></a>\n",
    "\n",
    "What is an epoch? For the purposes of the FCMA package, an epoch is a time-interval that you want to carve up for analysis. FCMA needs to know which timepoints in the data correspond to which events, much like using labels in the previous notebooks. In the connectivity notebook our epochs were the entire run of data but in the code here this is just each block of faces or scenes. The stimuli were shown only for 12 TRs (18s) followed by 6 TRs (12s) of rest.\n",
    "\n",
    "The epoch file has a very specific structure that makes it readible by BrainIAK. The epoch file is a list in which each entry is a 3d matrix of booleans. Each entry in this list corresponds to a participant/run (since there was one run per participant). The dimensions of the 3D matrix are condition (face or scene) by epoch (trial 1, 2, etc.) by TR.\n",
    "\n",
    "**Exercise 1:**<a id=\"ex1\"></a> Read in the epoch data from \"fs_epoch_labels.npy\". Use plt.imshow to display the epoch by TR sequence for the face and scene conditions in the first participant, separately. Make sure the figure is big enough to read."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2:**<a id=\"ex2\"></a> Create an epoch file that includes only the first 3 participants and use [`numpy.save`](https://docs.scipy.org/doc/numpy-1.14.0/reference/generated/numpy.save.html) to save it in NumPy fromat here: './face_scene/fs_epoch_labels_3sub.npy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's load in the new epoch file with the [io.load_labels tool](http://brainiak.org/docs/brainiak.html?highlight=io#module-brainiak.io). We can also pull out some useful information from the epoch file such as how many epochs there are per person."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/pytorch51/face_scene/fs_epoch_labels_3sub.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-67c1c75a9079>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mnew_epoch_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_dir\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'fs_epoch_labels_3sub.npy'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mepoch_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_epoch_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Parse the epoch data for useful dimensions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mepochs_per_subj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepoch_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/site-packages/brainiak/io.py\u001b[0m in \u001b[0;36mload_labels\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0mList\u001b[0m \u001b[0mof\u001b[0m \u001b[0mSingleConditionSpec\u001b[0m \u001b[0mstored\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m     \"\"\"\n\u001b[0;32m--> 142\u001b[0;31m     \u001b[0mcondition_specs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSingleConditionSpec\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcondition_specs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/site-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[1;32m    370\u001b[0m     \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbasestring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 372\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    373\u001b[0m         \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    374\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mis_pathlib_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/pytorch51/face_scene/fs_epoch_labels_3sub.npy'"
     ]
    }
   ],
   "source": [
    "new_epoch_file = data_dir + 'fs_epoch_labels_3sub.npy'\n",
    "epoch_list = io.load_labels(new_epoch_file)\n",
    "\n",
    "# Parse the epoch data for useful dimensions\n",
    "epochs_per_subj = epoch_list[0].shape[1]\n",
    "num_subjs = len(epoch_list)\n",
    "\n",
    "print('Number of participants:', num_subjs)\n",
    "print('Number of epochs per participant:', epochs_per_subj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Normalize data<a id=\"prep_for_fcma\"></a>\n",
    "\n",
    "BrainIAKs FCMA function takes in the brain images, mask and epoch file via the [prepare_fcma_data](http://brainiak.org/docs/brainiak.fcma.html?highlight=prepare_fcma#brainiak.fcma.preprocessing.prepare_fcma_data) function to format and normalize the data for analysis. You might wonder why this step is not included in the actual FCMA function. Why bother with a dedicated preparation step? The rationale behind this is that it helps in speeding up the correlation computation. As such it is a necessary step to use BrainIAKs optimized processing routines.\n",
    "\n",
    "Note that this function has a nice feature: if your input `epoch_list` has fewer entries than your input `images` then it will only take the first N images for which entries in the `epoch_list` exist. This applies to what we do here: we input an epoch file for three participants only, while inputting an BrainIAK image object with fMRI data for all participants. In other words, if the `epoch_list` is subsampled then your `images` will also be subsampled. However, make sure you have the order right: It will only take the first N participants so make sure your epoch file corresponds to those same participants.\n",
    "\n",
    "We are initially going to work with the first three participants to illustrate feature selection. This could take 20-30 mins on 2 cores. We will then use the results of feature selection for all participants (which we already calculated) to perform final classification. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize and format the data.\n",
    "raw_data, _, labels = prepare_fcma_data(images, epoch_list, mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 3:**<a id=\"ex3\"></a> What does each list element of raw_data and labels represent? What does the dimensionality of the array in each raw_data list element represent?\n",
    "\n",
    "**A:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 1.4 Compute correlations and voxel selection <a id=\"vox_sel\"></a>\n",
    "The computational workhorse behind FCMA is the code in [compute_correlation](http://brainiak.org/docs/brainiak.fcma.html?highlight=compute_correlation#brainiak.fcma.util.compute_correlation). This is C code written with [cython](http://cython.org/\n",
    ") (a python binding for C code) that allows for extremely efficient computation of correlations.\n",
    "\n",
    "The following example extracts the data for one epoch, for one subject and computes the correlation using this procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract data for one subject, for one epoch (take only the every 10th voxel)\n",
    "epoch_data = raw_data[0][:,::10]\n",
    "\n",
    "# Make the data c contiguous (the type of ordering used in C in which the last dimensions are stored first (opposite of fortran/matlab))\n",
    "mat = np.ascontiguousarray(epoch_data.T, dtype=np.float32)  # Voxels x TRs for one epoch\n",
    "\n",
    "begin_time = time.time()\n",
    "epoch_corr = compute_correlation(mat, mat)  # correlation of every voxel with every other voxel\n",
    "end_time = time.time()\n",
    "print(\"Analysis duration: %0.5fs\" % (end_time - begin_time))\n",
    "\n",
    "plt.figure()\n",
    "plt.title('Correlations for one epoch');\n",
    "plt.xlabel('voxels');\n",
    "plt.imshow(epoch_corr);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 4:**<a id=\"ex4\"></a> What are the inputs to the [compute_correlation](http://brainiak.org/docs/brainiak.fcma.html?highlight=compute_correlation#brainiak.fcma.util.compute_correlation) function and what is the main trick this function pulls to speed up computation?  \n",
    "\n",
    "**A:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Voxel Selection**\n",
    "\n",
    "To understand how voxel selection in FCMA works, remember what we did in week five's excercise on classifier optimization in which we learned different ways to perform cross-validation. One option is to leave out some unit or units of your data such as a run or a participant while you fit parameters or select voxels. In FCMA, it is typical to perform nested cross-validation in which we leave out a participant for final testing on an outer loop (e.g., participant 18) and perform our voxel selection with cross-validation on an inner loop with a participant left out to quantify feature selection performance (e.g., participant 17) and the rest used to conduct feature selection (e.g., participants 1-16). Thus, for each of 18 outer-loop folds, there are 17 inner-loop folds. To use this approach we simply ignore one subject's data before executing the voxel selection procedure (the inner loop itself). We can then use that data for final classifier testing. Then, on new iterations of the outer loop we can rotate which participants are to be ignored.\n",
    "\n",
    "The inner loop, i.e. the voxel selection procedure, consists of computing correlation matrices and the classification of correlation patterns. This is highly computationally demanding and has been optimized in BrainIAK, where the [VoxelSelector](http://brainiak.org/docs/brainiak.fcma.html?highlight=voxel%20selector#module-brainiak.fcma.voxelselector) method implements this massive computation (all voxels x all subjects x all epochs). How does it work?\n",
    "\n",
    "1. First, [VoxelSelector](http://brainiak.org/docs/brainiak.fcma.html?highlight=voxel%20selector#module-brainiak.fcma.voxelselector) uses the [compute_correlation](http://brainiak.org/docs/brainiak.fcma.html?highlight=compute_correlation#brainiak.fcma.util.compute_correlation) function to compute the correlation of every voxel with every other voxel in the mask provided for each epoch and participant in the training set. \n",
    "2. It then trains a classifier for each voxel on a training set consisting of all epochs of n-1 participants. Each voxel's classifier uses the voxel's correlation values with all other voxels in the mask as features (as if it were a seed). \n",
    "3. Finally, it tests the classifier on the nth participant in the training set. \n",
    "\n",
    "This is repeated n times as a result of cross-validation and the accuracies are averaged across folds for each voxel. We can then rank voxels in terms of performance and use this ranking as a way to select which voxels have discriminative connectivity in the training set.\n",
    "\n",
    "**Exercise 5:**<a id=\"ex5\"></a> Excluding one participant from the training set is arbitrary: we could leave out some blocks from all participants, multiple participants or any combination of these choices (as long as the training and testing data are independent). In the space below, write code using the `raw_data` and `labels` variables created by `prepare_fcma_data` to input into VoxelSelector with N-2 participants (i.e., 2 participants held out for final classifier testing). Print out the new shape of `raw_data` and `labels` to confirm your answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How FCMA works on a computer or cluster**\n",
    "\n",
    "The FCMA implementation in BrainIAK efficiently parallelizes and dessiminates the computation amongst the resources available. Below is a figure detailing the workflow of FCMA. The figure caption is quoted below:\n",
    "\n",
    "![image](https://ars.els-cdn.com/content/image/1-s2.0-S0165027015001910-gr1.jpg)\n",
    ">\"Fig. 1. Workflow overview. FCMA uses a controller/worker architecture, in which each worker first loads the full data into memory. The full data consist of a matrix with V voxels in rows and T timepoints in columns; the timepoints can be subdivided into E epochs, each with TE timepoints (inset depicts two voxels and epochs). The controller process does the following: assigns a subset S of voxels to each of W workers; instructs the worker to compute the correlation between each of these voxels and the rest of the brain in each epoch; instructs the worker to analyze the correlation vectors for each voxel across epochs with MVPA and supplied condition labels; collects the analysis result (i.e., cross-validation accuracy) for each voxel and loads it into memory; and returns to the first step to assign another subset of voxels until there are none left. Finally, the controller writes the results to disk.\" Source:  [Wang et al. (2015)](https://doi.org/10.1016/j.jneumeth.2015.05.012)\n",
    "\n",
    "This computation cannot be run within a notebook because the controller and worker relationship needs at least two processors to run; whereas we are currently only using one for this notebook.\n",
    "\n",
    "Instead we provide some [batch scripts](#fcma_batch) to execute the whole sequence of the FCMA workflow. However, the code isn't scary and actually does something quite similar to what we have done before with [scikit-learn](http://scikit-learn.org/stable/) for classification: we create a classification object with the inputs we desire, specify the classification kernel and then run it, like this:\n",
    "\n",
    ">vs = VoxelSelector(labels, epochs_per_subj, num_subjs, raw_data)  \n",
    ">clf = SVC(kernel='precomputed', shrinking=False, C=1)  \n",
    ">results = vs.run(clf) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Create voxel selection masks <a id=\"masks\"></a>\n",
    "\n",
    "When we have each voxel labeled in terms of its average accuracy across inner loop folds we can then choose how many voxels we wish to include in building our classifier model to test the left-out outer-loop participant. This is a feature selection step just like we did with classifiers in notebook 03. Unlike there, where it was relatively easy to do feature selection, here the analyses are very long and so we need to save our intermediate steps. Regardless, the logic is the same: find the voxels that we think will be most useful in our classification for a held out, never before seen, classification. \n",
    "\n",
    "The bash script we use here calls [FSL](https://fsl.fmrib.ox.ac.uk/fsl/fslwiki/FSL) commands to choose a subset of voxels (top N) and create a binary mask for each training set/outer-loop fold. As you should be able to see, it creates three types of files.\n",
    "1. For each input volume, a mask for the top N voxels is created (`${pref}_top${voxel_number}.nii.gz`)\n",
    "2. These individual masks are then concatenated, resulting in an array in which each entry is a top-N-voxel volume mask (`all_top\\${voxel_number}.nii.gz`)\n",
    "3. This concatenated array is used to create one map which gives the probability for each voxel to be included in the top N (`prop_top\\${voxel_number}.nii.gz`) \n",
    "\n",
    "**Self-study:** Try to understand what happens in the script above. Here are some hints: \\${voxel_number} is the number of voxels included. \\$subj is the outer-loop subject number. `fslmerge` has the following input structure: 'fslmerge -t \\$output_name \\$input_1 \\$input_2 ... $input_N'. The last line that calls *fslmaths* creates the prop_top volume by doing a computation on the all_top volumes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat ./09-fcma/make_top_voxel_mask.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6 Classification <a id=\"classify\"></a>\n",
    "\n",
    "On this subset of top N voxels we perform classification of the correlation patterns by first training on the subjects we used for voxel selection (all participants in the training set) and then testing on the held-out outer-loop participant that wasn't used for any previous analyses.\n",
    "\n",
    "BrainIAK has a special method, called [Classifier](http://brainiak.org/docs/brainiak.fcma.html?highlight=classifier#module-brainiak.fcma.classifier), that will compute correlations on the selected voxels and perform classification in a computationally efficient manner. We have created the script *fcma_classify.py* as a wrapper. This script is similar to the voxel selection script: data are loaded into memory on each processor, then normalized and prepared for input to FCMA, a *Classifier* object is made from the data and then fit. Like voxel selection, this can be called in a similar way to the classification tools from [scikit-learn](http://scikit-learn.org/stable/).\n",
    "\n",
    "Critically, the mask input to the *fcma_classify.py* is not the whole brain mask but is instead the top N voxels that are deemed appropriate for this outer-loop fold.\n",
    "\n",
    "As before the code is relatively tractable and familiar; however, again we cannot run this in the notebook because of the need for parallelism. Once our data and labels are arranged into training and test sets we create objects that are ready to be read by FCMA. We then create our SVM kernel and Classifier object. This Classifier object is then fit to the training data before being tested on the never-before-seen test data.\n",
    "\n",
    ">training_obj = list(zip(training_data, training_data))  \n",
    ">testing_obj = list(zip(testing_data, testing_data))  \n",
    "\n",
    ">svm_clf = SVC(kernel='precomputed', shrinking=False, C=1)  \n",
    ">clf = Classifier(svm_clf, epochs_per_subj=epochs_per_subj)  \n",
    "\n",
    ">clf.fit(training_obj, training_labels)  \n",
    ">predict = clf.predict(testing_obj)  \n",
    "\n",
    "If you look at the *fcma_classify.py* script you will notice that there is an alternative way to organize the data and feed it into the classifier function. If you concatenate the training and testing data then zip it into a single object, this is much more memory efficient. The FCMA code then takes account of how many training samples there are and *never* looks at the test data when fitting the model. This procedure is thus better, although a little harder to understand. \n",
    "\n",
    "Users of FCMA have found it useful to distinguish between intrinsic and extrinsic classification. \n",
    "\n",
    "> Intrinsic classification, demonstrated above, is when the correlations of only the voxels in the top_n_mask are used for classification. In other words this method only cares about correlations among those voxels that were selected because they contain information.  \n",
    "\n",
    "> Extrinsic classification is when the correlations used for final classification are between voxels within the top_n_mask and voxels outside of this mask. In other words this method examines information captured by these nodes with the rest of the brain.\n",
    "\n",
    "The scripts we provide allow you to specify whether you wish to perform intrinsic or extrinsic FCMA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. FCMA Batch Scripts <a id=\"fcma_batch\"></a>\n",
    "\n",
    "We have covered the main functions needed to run a FCMA analysis. Some of these methods require multiple cores for fast execution and thus cannot be executed from cells in a Jupyter notebook. The scripts are described below.\n",
    "\n",
    "Note that if you run the scripts on milgram but do not participate in the course, you will run into this error at some point: 'sbatch: error: Batch job submission failed: Invalid account or account/partition combination specified'. This happens because the .sh-scripts are set up to run on a dedicated partition for the course. To solve the error, change the .sh-script file (e.g. using the nano command) and change the line '#SBATCH --partition cmhn-s18' to '#SBATCH --partition short; and delete the line '#SBATCH -A cmhn-s18' before running the scripts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**./09-fcma/run_fcma_voxel_selection_cv.sh** \n",
    "\n",
    "This runs *./09-fcma/fcma_voxel_selection_cv.py* which loads in and formats the [fMRI](#data_prep) and [epoch](#epoch) data, [performs normalization](#prep_for_fcma), and [VoxelSelection](#vox_sel). The bash script takes in six inputs:\n",
    "1. data_dir=What is the directory containing data? \n",
    "        e.g. \"./face_scene/\"\n",
    "2. suffix=What is the extension of the data you're loading \n",
    "        e.g. \".nii.gz\"\n",
    "3. mask_file=What is the path to the whole brain mask \n",
    "        e.g. \"./face_scene/mask.nii.gz\"\n",
    "4. epoch_file=What is the path to the epoch file \n",
    "        e.g. \"./face_scene/fs_epoch_labels_3sub.npy\"\n",
    "5. left_out_subj=Which participant (as an integer) are you leaving out for this cv? \n",
    "        e.g. \"0\"\n",
    "6. output_dir=Where do you want to save the data\n",
    "        e.g. \"./09-fcma/voxel_selection_subsample\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**./09-fcma/make_top_voxel_mask.sh** \n",
    "\n",
    "[Creates binary masks of the top N voxels](#masks) for each file generated by voxel selection in a given folder. This creates a mask for each participants and then aggregate masks. This takes the following inputs:\n",
    "1. input_dir=What is the path to the data?\n",
    "        e.g. \"./09-fcma/voxel_selection_subsample/\"\n",
    "2. voxel_number=What voxel threshold would you like to set\n",
    "        e.g. \"100\"\n",
    "3. output_dir=Where do you want to put the data\n",
    "        e.g. \"./09-fcma/top_n_masks_subsample/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**./09-fcma/run_fcma_classify.sh**\n",
    "\n",
    "This runs *fcma_classify.py* which performs [classification](#classify) of the voxels selected by VoxelSelection, using the *Classifier* method. This takes the following inputs:\n",
    "1. data_dir=What is the directory containing data? \n",
    "        e.g. \"./face_scene/\"\n",
    "2. suffix=What is the extension of the data you're loading \n",
    "        e.g. \".nii.gz\"\n",
    "3. top_n_mask_file=What is the path to the top N mask file (*this is not the whole-brain mask*) \n",
    "        e.g. \"./09-fcma/top_n_masks_all/fc_no0_result_seq_top1000.nii.gz\"\n",
    "4. epoch_file=What is the path to the epoch file \n",
    "        e.g. \"./face_scene/fs_epoch_labels.npy\"\n",
    "5. left_out_subj=Which participant (as an integer) are you using for testing? \n",
    "        e.g. \"0\"\n",
    "6. second_mask=Do you want to use a second mask to compare the data with? Necessary for extrinsic analyses. Otherwise set to None.\n",
    "        e.g. \"None\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Inner loop (subsample)<a id=\"fcma_inner\"></a>\n",
    "\n",
    "Although the FCMA tools have greatly sped up the computation time required for these analyses, it still takes a long time to compute a trillion (that's 1,000,000,000,000) correlations, as is needed for voxel selection with this dataset. Hence we are only going to run these voxel selection on three participants.\n",
    "\n",
    "**Exercise 6:**<a id=\"ex6\"></a> Perform the following steps for your toy-sample of three participants:  \n",
    "1. Check that you have created the epoch file with the first three participants in [Excercise 2](#ex2) and it is called \"./face_scene/fs_epoch_labels_3sub.npy\"\n",
    "2. Run the voxel selection (`sbatch ./09-fcma/run_fcma_voxel_selection_cv.sh`), leaving one of the participants out each time. You can use the example inputs specified above for the first participant. This can take a couple of minutes with only two cores.\n",
    "3. Create a mask of the top **100** voxels (`sbatch ./09-fcma/make_top_voxel_mask.sh`. You can use the example inputs specified above. This will only take seconds. Remember that this script uses [FSL](https://fsl.fmrib.ox.ac.uk/fsl/fslwiki/FSL) fuctions. Thus, it will only work if you have set up and sourced [FSL](https://fsl.fmrib.ox.ac.uk/fsl/fslwiki/FSL).\n",
    "4. Plot the mask of the top **100** voxels for each participant (saved in `./09-fcma/top_n_masks_subsample`) using the [nilearn.plotting](http://nilearn.github.io/plotting/index.html) tools you discovered in week 08."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Insert code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Outer loop (all)<a id=\"fcma_outer\"></a>\n",
    "\n",
    "The above exercise was to familiarize you with how the voxel selection step works. This step is slow because we are calculating the full correlation matrix and a separate cross-validation is performed for every voxel's connectivity. However, the outer loop is relatively fast, at least for intrinsic analyses, because we are only training and testing on a subset of voxels. Hence we will use voxel selection data we prepared earlier (`/gpfs/milgram/data/cmhn-s18/datasets/face_scene/voxel_selection_all/`) to run the outer loop aka final classification for each participant: Note that the selected features will be slightly different for each outer-loop participant because the training sets differ. \n",
    "\n",
    "If you don't have access to this data then you will need to run this yourself. Simply create an epoch file with all participants included and run the [voxel selection bash script](#fcma_batch) again (create and use a new output directory for this: `/09-fcma/voxel_selection_all`). This will take considerably longer than what we did for only three subjects before. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ls /gpfs/milgram/data/cmhn-s18/datasets/face_scene/voxel_selection_all/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Self-study:** Visualize the masks that were previously created. Note, these masks will be different from the ones that you created because they are trained on substantially more data. You can use the code: \n",
    ">`module load Apps/FSL/5.0.9`\n",
    "\n",
    ">`fslview /gpfs/milgram/data/cmhn-s18/datasets/face_scene/voxel_selection_all/fc_no0_result_score.nii.gz`\n",
    "\n",
    "**Exercise 7:**<a id=\"ex7\"></a> Create a mask of the top **1000** voxels using the `make_top_voxel_mask.sh` script. Use the `voxel_selection_all` folder (full path above) as an input and `./09-fcma/top_n_masks_all` as the output destination.\n",
    "\n",
    "**Exercise 8:**<a id=\"ex8\"></a> Perform intrinsic classification on all participants using the `run_fcma_classify.sh` script. You can use all of the example inputs specified above. Once this has finished, you can print out the 18 outer-loop classification accuracies in the cell below. Use these to calculate the overall mean performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat classify_result.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Insert code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Permutation testing <a id=\"fcma_perm\"></a>\n",
    "\n",
    "FCMA also has tools to allow for easy permutation of data. This way you can determine, by running this analysis many times, the distribution of classification accuracy for a null effect and how the real effect stacks up. The code below uses the [prepare_fcma_data](http://brainiak.org/docs/brainiak.fcma.html?highlight=prepare_fcma#brainiak.fcma.preprocessing.prepare_fcma_data) function we already know by using its *random argument* to perform permutation (of the image data, not the labels). Setting the random argument produces random voxel selection results for non-parametric statistical analysis. There are three random options:\n",
    "1. RandomType.NORANDOM is the default\n",
    "2. RandomType.REPRODUCIBLE permutes the voxels in the same way every run\n",
    "3. RandomType.UNREPRODUCIBLE permutes the voxels differently across runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that you must run this every time you wish to call prepare_fcma_data\n",
    "images = io.load_images_from_dir(data_dir, suffix)\n",
    "\n",
    "# include the random argument\n",
    "permuted_raw_data, _, permuted_labels = prepare_fcma_data(images, epoch_list, mask, random=RandomType.REPRODUCIBLE)\n",
    "print(permuted_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 9:**<a id=\"ex9\"></a> Perform a permutation analysis by doing the following steps:\n",
    "1. Create a new version of the *fcma_classify.py* script that permutes the data (like the code above) and name it *fcma_classify_permuted.py*.\n",
    "2. Have this script output the accuracy to a text file called *classify_result_permuted.txt*.\n",
    "3. Create a new version of the *run_fcma_classify.sh* script that executes *fcma_classify.py* and name it *run_fcma_classify_permuted.sh*. \n",
    "4. On the first outer-loop participant, run this analysis 10 times to get a distribution of null classification accuracies. Print the results below and calculate the mean.\n",
    "\n",
    "*Note that as of April 2018 there seemed to be a bug in the `random` argument of the `prepare_fcma_data` function, so there was no effect of permutation. An [issue](https://guides.github.com/features/issues/) was raised on the [BrainIAK github repository](https://github.com/brainiak/brainiak) and will hopefully resolved soon.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Insert code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat classify_result_permuted.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Plotting the results <a id=\"plot\"></a>\n",
    "\n",
    "As always, it is useful to visualize our results to get an idea of where in the brain the contains information. However, in the case of FCMA, remember the information isn't localized to a voxel but instead is captured in the relationship between voxel. Hence we typically want to plot a connectome rather than a heatmap. Fortunately, as you might remember from week 08, there are great tools for [plotting connectomes in nilearn](http://nilearn.github.io/modules/generated/nilearn.plotting.plot_connectome.html).\n",
    "\n",
    "### 3.1 Plot the connectome <a id=\"connectome\"></a>\n",
    "\n",
    "Below we are going to load in a mask that we have created and then compute correlations of every voxel in the mask with every other voxel in the mask (intrinsic). We are then going to create a connectome based on the strongest correlations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in the data\n",
    "epoch_data = raw_data[0] # Just load a single subject and a single epoch\n",
    "mask_top_n_file = './09-fcma/top_n_masks_all/fc_no0_result_seq_top1000.nii.gz'\n",
    "mask_top_n_nii = nib.load(mask_top_n_file)  # Load the mask that leaves that participant out\n",
    "mask_top_n = mask_top_n_nii.get_data()\n",
    "\n",
    "# Convert the top n mask into a vector with the same number of elements as the whole brain\n",
    "mask_vec = mask.reshape(np.prod(mask.shape))\n",
    "mask_top_vec = mask_top_n.reshape(np.prod(mask_top_n.shape))[mask_vec]\n",
    "\n",
    "# Mask the epoch data\n",
    "epoch_data_masked = epoch_data[:, mask_top_vec==1]\n",
    "\n",
    "# Make the data c continguous \n",
    "epoch_data_masked = np.ascontiguousarray(epoch_data_masked.T, dtype=np.float32)\n",
    "\n",
    "# Create the internal correlation\n",
    "epoch_corr = compute_correlation(epoch_data_masked, epoch_data_masked)\n",
    "\n",
    "# Pull out the coordinates of the mask (in numpy space)\n",
    "coord_x, coord_y, coord_z = np.where(mask_top_n == 1)\n",
    "\n",
    "# Convert from the input space into the same space as the mask_top_n_nii file\n",
    "# This is not MNI space, so the output will not match the glass brain \n",
    "coords = coord_transform(coord_x, coord_y, coord_z, mask_top_n_nii.affine)\n",
    "\n",
    "# Save for later\n",
    "np.save(\"epoch_corr\", epoch_corr)\n",
    "np.save(\"epoch_corr_coords\", coords)\n",
    "\n",
    "# Plot the connectome (only connections in above the specified percentile will be plotted)\n",
    "plotting.plot_connectome(epoch_corr, \n",
    "                         np.transpose(coords), \n",
    "                         edge_threshold=\"99.9%\", \n",
    "                         node_size=1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Plotting circos <a id=\"circos\"></a>\n",
    "\n",
    "As you can see, the plot above is not very informative due to its high number of nodes. So we need other plotting tools for visualizing our complex data. Circos is a very good one for our purpose here and this is what it looks like: if someone tells you that science isn't art then that person has never seen a circos plot:\n",
    "\n",
    "![image](http://mkweb.bcgsc.ca/tableviewer/userimg/circos-table-fdnrtsf.jpg)\n",
    "\n",
    "How does it work? Circos plots take in a correlation matrix and treat each row/column of the matrix as a point around the circle. They then draw a connection to points that exceed a certain threshold. These plots can then be used to display high-dimensional ROI data like you saw above.\n",
    "\n",
    "**Self-study:** This [website](http://mkweb.bcgsc.ca/tableviewer/visualize/) allows you to upload a table of data and make a circos plot. If you would like, you can use the script below to save a correlation matrix as a table that can be read into the website. You could then upload that into your git repo and render it in this notebook or you could download the file directly from the repo. Mileage may vary with the website but when it works it is great. Note that you cannot have a matrix that is bigger than 75 * 75 so figure out how to select your data first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Print circos compatibile information\n",
    "fid = open('circos_table.txt', 'w')\n",
    "for voxel_row in range(epoch_corr.shape[0]):\n",
    "    \n",
    "    # Do something different for the first row\n",
    "    if voxel_row == 0:\n",
    "        line = 'Labels\\t'\n",
    "\n",
    "        for voxel_col in range(epoch_corr.shape[1] - 1):\n",
    "            line += \"vox_\" + str(voxel_col) + '\\t'\n",
    "    else:\n",
    "        \n",
    "        # Pull out the label and the content of the first line\n",
    "        line = \"vox_\" + str(voxel_row -1) + '\\t'\n",
    "        for voxel_col in range(epoch_corr.shape[1]):\n",
    "            weight = int(abs(epoch_corr[voxel_row - 1, voxel_col]*100))  # Must be positive integers\n",
    "            line += str(weight) + '\\t'\n",
    "    \n",
    "    # Write the line you have prepared\n",
    "    fid.write(line + '\\n')\n",
    "\n",
    "# Close the text file that was created\n",
    "fid.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# displaying this file might take a while, as it has a lot of entries\n",
    "!cat circos_table.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Circos packages are becoming more available now in the python community. For those really interested you should look at the [nxviz](https://github.com/ericmjl/nxviz) github repository. \n",
    "\n",
    "**Self-study:** On milgram a module for this has been created to let you make circos plots. However, to be able to use it you need to load different modules (the [nxviz](https://github.com/ericmjl/nxviz) package has different dependencies than BrainIAK). So, do not attempt to re run everything in this notebook, the circos environment does not have brainiak. Instead, open a new tunnel with a new environment and run the contents of the cell below and it will work (assuming every other exercise in this notebook is done).\n",
    "To do this, \n",
    "1. run `sbatch ./09-fcma/run_jupyter_circos.sh` to set up an appropriate environment.\n",
    "2. Note the JOBID that was assigned to your job (\"Submitted batch job JOBID\") in your terminal and open the corresponding .txt file (jupyter-log-JOBID.txt) to see the instructions of opening a new tunnel to run your notebook through (analog to what you do after executing ./launch_jupyter.sh)\n",
    "3. You might be prompted to provide a password or token. The token is also printed in the jupyter-log-JOBID.txt file (in the last line, everything after \"jupyter-log-JOBID.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary libraries (and not extra)\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from nxviz.plots import CircosPlot\n",
    "%matplotlib inline\n",
    "\n",
    "# What is the (absolute) correlation threshold\n",
    "threshold = 0.75\n",
    "\n",
    "# Load in the data\n",
    "epoch_corr = np.load(\"epoch_corr.npy\")\n",
    "epoch_coords = np.load(\"epoch_corr_coords.npy\")\n",
    "\n",
    "# Preset the graph\n",
    "G = nx.Graph()\n",
    "\n",
    "# Create the edge list\n",
    "nodelist = []\n",
    "edgelist = []\n",
    "for row_counter in range(epoch_corr.shape[0]):\n",
    "    nodelist.append(str(row_counter))  # Set up the node names\n",
    "    \n",
    "    for col_counter in range(epoch_corr.shape[1]):\n",
    "        \n",
    "        # Determine whether to include the edge based on whether it exceeds the threshold\n",
    "        if abs(epoch_corr[row_counter, col_counter]) > threshold:\n",
    "            # Add a tuple specifying the voxel pairs being compared and the weight of the edge\n",
    "            edgelist.append((str(row_counter), str(col_counter), {'weight': epoch_corr[row_counter, col_counter]}))\n",
    "        \n",
    "# Create the nodes in the graph\n",
    "G.add_nodes_from(nodelist)\n",
    "\n",
    "# Add the edges\n",
    "G.add_edges_from(edgelist)\n",
    "\n",
    "# Set the colors and grouping (specify a key in a dictionary that can then be referenced)\n",
    "for n, d in G.nodes(data=True):\n",
    "    \n",
    "    # Is the x coordinate negative (left)\n",
    "    if epoch_coords[0][int(n)] < 0:\n",
    "        if epoch_coords[1][int(n)] < 0:\n",
    "            G.node[n]['grouping'] = 'posterior_left'\n",
    "        else:\n",
    "            G.node[n]['grouping'] = 'posterior_right'\n",
    "    else:\n",
    "        if epoch_coords[1][int(n)] < 0:\n",
    "            G.node[n]['grouping'] = 'anterior_left'\n",
    "        else:\n",
    "            G.node[n]['grouping'] = 'anterior_right'\n",
    "\n",
    "# plot the data\n",
    "c = CircosPlot(graph=G, node_grouping='grouping', node_color='grouping')\n",
    "c.draw()\n",
    "plt.title('Circos plot of epoch data');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. MVPA and FCMA<a id=\"mvpa\"></a>\n",
    "\n",
    "The FCMA workflow is intentionally set up in BrainIAK to be parallel to MVPA. Rather than looking at correlations, this simply uses patterns of activity across different trials to discriminate conditions. As before there are scripts available for voxel selection and final classification. Here voxel selection is based on a searchlight analysis in n-1 participants.\n",
    "\n",
    "**Self-study:** With a few line changes it is possible to run MVPA with the code from [section 2](#fcma_batch). Investigate what you need to change to make this happen, and feel free to run this analysis (at least on a subset of participants) for your novel contribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Novel contribution:** <a id=\"novel\"></a> be creative and make one new discovery by adding an analysis, visualization, or optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contributions <a id=\"contributions\"></a>\n",
    "\n",
    "B. Hutchinson provided data  \n",
    "B. Hutchinson and Y. Wang provided initial code  \n",
    "M. Kumar, C. Ellis and N. Turk-Browne produced the initial notebook 3/27/18  "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
