{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "4988daf4-346a-4a15-b8f8-4802ccdfb87b"
    }
   },
   "source": [
    "# Classifier Optimization\n",
    "**V.0.1 - Alpha testing, [contributions](#contributions)**\n",
    "\n",
    "In earlier exercises, we explored a variety of classifiers and feature selection techniques. During the past exercises we didn't pay much attention to the parameters of these procedures and set them arbitrarily or based on intuition. In what follows we are going to investigate data-driven, unbiased techniques to optimize classification pipelines.\n",
    "\n",
    "Cross-vaidation can allow us to pick the best performing parameters in in an unbiased fashion. We will be using the useful features from scikit-learn to build up some cross-validation analyses. scikit-learn also offers a simple procedure for building and automating the various steps involved in classifier optimization (e.g. data scaling => feature selection => parameter tuning). This is part of the [Pipeline package](http://scikit-learn.org/stable/modules/pipeline.html#pipeline). We will also explore these methods in this exercise.\n",
    "\n",
    "## Goal of this script\n",
    "1. Build a pipeline of steps to optimize classifier performance.    \n",
    "2. Use the pipeline to make optimal choices.\n",
    "\n",
    "**Recap:** The localizer data we are working with ([Kim et al., 2017](https://doi.org/10.1523/JNEUROSCI.3272-16.2017)) consists of 3 runs with 5 blocks for each category. In the matlab stimulus file, the first row has the stimulus labels for the 1st, 2nd and 3rd runs of the localizer. Each run was 310 TRs.\n",
    "The 4th row contains the time when the stimulus was presented for each of the runs. The stimulus labels and their corresponding categories are as follows: 1 = Faces, 2 = Scenes, 3 = Objects\n",
    "\n",
    "\n",
    "## Table of Contents\n",
    "[1. Make preprocessing pipeline](#preprocessing)  \n",
    "\n",
    "[2. How big should a training set be?](#training)  \n",
    "\n",
    "[3. Cross-validation: Hyper-parameter selection](#cross_val)   \n",
    "\n",
    "[4. How to avoid double dipping](#double_dipping)  \n",
    ">[4.1 Example of double dipping](#example_double_dip)  \n",
    "\n",
    "[5. Make a pipeline](#pipeline)  \n",
    "\n",
    "Exercises\n",
    ">[Exercise 1](#ex1)  \n",
    ">[Exercise 2](#ex2)  \n",
    ">[Exercise 3](#ex3)  \n",
    ">[Exercise 4](#ex4)  \n",
    ">[Exercise 5](#ex5)  \n",
    ">[Exercise 6](#ex6)  \n",
    ">[Exercise 7](#ex7)  \n",
    ">[Exercise 8](#ex8)  \n",
    ">[Exercise 9](#ex9)  \n",
    "\n",
    "[Novel contribution](#novel)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "1f7f9d75-833f-410f-8988-58c1618fa753"
    }
   },
   "outputs": [],
   "source": [
    "# Import fMRI and general analysis libraries\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "from scipy import stats\n",
    "import pandas as pd\n",
    "\n",
    "# Import plotting library\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib notebook\n",
    "\n",
    "# Import machine learning libraries\n",
    "from nilearn.input_data import NiftiMasker\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.pipeline import Pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 1. Make preprocessing pipeline <a id=\"preprocessing\"></a>\n",
    "\n",
    "In past notebooks we have preprocessed the fMRI data from [Kim et al. (2017)](https://doi.org/10.1523/JNEUROSCI.3272-16.2017) using the following steps:  \n",
    ">Extract the BOLD data for a mask.  \n",
    ">Get the the stimulus labels.  \n",
    ">Assign a label to every TR.  \n",
    ">Shift the label time course to take account of the hemodynamic lag.  \n",
    ">Extract BOLD data only for the conditions of interest (ignore the TRs corresponding to the baseline, i.e. when there was no task).  \n",
    ">Average stimuli within blocks in order to reduce concerns around temporal autocorrelation.\n",
    "\n",
    "In general it can be useful to make a script that contains all of the functions you might use across multiple scripts. This is so that if you make an update to the function, you don't have to update all of the versions in the scripts that might otherwise define the function. Often these will be python scripts called *utils.py*\n",
    "\n",
    "**Self-Study:** Explore the *utils.py* script to see how it is possible to make this kind of script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We still have to import the functions of interest\n",
    "from utils import load_data, load_labels, label2TR, shift_timing, reshape_data, blockwise_sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the data from one participant ready for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Preset variables\n",
    "dir = '/opt/public_FMRI/vdc/'\n",
    "num_runs=3\n",
    "TR=1.5\n",
    "hrf_lag = 4.5  # In seconds what is the lag between a stimulus onset and the peak bold response\n",
    "shift_size = int(hrf_lag / TR)  # Convert the shift into TRs\n",
    "\n",
    "sub_id = 1\n",
    "\n",
    "# Convert the number into a participant folder name\n",
    "if (sub_id < 10):\n",
    "    sids = '0' + str(sub_id)\n",
    "else:\n",
    "    sids = str(sub_id)   \n",
    "\n",
    "# Specify the subject name\n",
    "sub = 'sub-' + sids\n",
    "\n",
    "# Load subject labels\n",
    "stim_label_allruns = load_labels(dir, sub)\n",
    "\n",
    "# Load the fMRI data using a whole-brain mask\n",
    "epi_mask_data_all, _ = load_data(directory=dir, subject_name=sub, mask_name='', zscore_data=True)\n",
    "\n",
    "# This can differ per participant\n",
    "print(sub, '= TRs: ', epi_mask_data_all.shape[1], '; Voxels: ', epi_mask_data_all.shape[0])\n",
    "TRs_run = int(epi_mask_data_all.shape[1] / num_runs)\n",
    "\n",
    "# Convert the timing into TR indexes\n",
    "stim_label_TR = label2TR(stim_label_allruns, num_runs, TR, TRs_run)\n",
    "\n",
    "# Shift the data some amount\n",
    "stim_label_TR_shifted = shift_timing(stim_label_TR, shift_size)\n",
    "\n",
    "# Perform the reshaping of the data\n",
    "bold_data, labels = reshape_data(stim_label_TR_shifted, epi_mask_data_all)\n",
    "\n",
    "# Down sample the data to be blockwise rather than trialwise\n",
    "bold_data, labels = blockwise_sampling(bold_data, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. How big should a training set be? <a id=\"training\"></a>\n",
    "\n",
    "When we split up our data into training and test sets we are trying to strike a balance between giving our classifier enough data to train a model with precise parameter estimates while ensuring that we also have enough data so that our test statistic has low variance. But what is that balance? If you google that, the common answer that you will find is: It depends! Generally, we use a rule of thumb that between 10% and 20% of our dataset should be the test. However, let's now investigate how different training set size affects classifier performance in a data-driven manner!\n",
    "\n",
    "Aside: Not only do your training samples need to be independent, but so do your test samples. If the test samples are highly correlated then the effective number of test samples is lower and the test statistic variance will be higher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Run a basic n fold classification\n",
    "def classification(classifier, data, labels, n_folds=5, test_size=0.2):\n",
    "    \n",
    "    # How many folds of the classifier\n",
    "    skfold = StratifiedShuffleSplit(n_splits=n_folds, test_size=test_size) \n",
    "\n",
    "    clf_score = np.array([])\n",
    "    for train, test in skfold.split(data, labels):\n",
    "\n",
    "        # Pull out the sample data\n",
    "        train_data = data[train, :]\n",
    "        test_data = data[test, :]\n",
    "        \n",
    "        # Train and test the classifier\n",
    "        clf = classifier.fit(train_data, labels[train])\n",
    "        clf_score = np.hstack((clf_score, clf.score(test_data, labels[test])))\n",
    "\n",
    "    return clf_score.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1:**<a id=\"ex1\"></a> Use a Support Vector Machine Classifier to examine how the accuracy of the classifier changes with different test set sizes from 10% to 90% in 10% steps. Plot the results. Do this over 10 folds to decrease the variability of the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "eee6bfd7-e6b6-4860-8501-6a3799dba268"
    }
   },
   "source": [
    "## 3. Cross-Validation: Hyper-parameter selection <a id=\"cross_val\"></a>\n",
    "\n",
    "Each of the classifiers we have used so far has one or more \"hyper-parameters\" used to configure and optimize the model based on the data and our goals. Read [this Machine Learning Mastery Article](https://machinelearningmastery.com/difference-between-a-parameter-and-a-hyperparameter/) for an explanation of the distinction between hyper-parameters and parameters. For instance, regularized logistic regression has a \"penalty\" hyper-parameter, which determines how much to emphasize the weight regularizing expression (e.g., L2 norm) when training the model.\n",
    "\n",
    "**Exercise 2:**<a id=\"ex2\"></a> SVM has a \"cost\" hyper-parameter, aka soft-margin hyper-parameter. Briefly describe what it does:\n",
    "\n",
    "**A:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to pick the best cost hyper-parameter for our dataset and to do this we will use cross-validation. Each hyper-parameter can be considered as a dimension such that the set of hyper-parameters is a space to be searched for effective values. The [GridSearchCV method in scikit-learn](http://scikit-learn.org/stable/modules/grid_search.html#grid-search) explores this space by dividing it up into a grid of values to be searched exhaustively. \n",
    "\n",
    "To give you an intuition for how grid search works, imagine trying to figure out what climate you find most comfortable. Let's say that there are two (hyper-)parameters that seem relevant: temperature and humidity. A given climate can be defined by the combination of values of these two parameters and you could report how comfortable you find this climate. A grid search would involve changing the value of each parameter with respect to the other in some fixed step size (e.g., 60 degrees and 50% humidity, 60 degrees and 60% humidity, 65 degrees and 60% humidity, etc.) and evaluating your preference for each combination.  \n",
    "\n",
    "Note that the number of steps and hyper-parameters to search is up to you. But be aware of combinatorial explosion: the granularity of the search (the smaller the steps) and the number of hyper-parameters considered increases the search time exponentially.\n",
    "\n",
    "GridSearchCV is an *extremely* useful tool for [hyper-parameter optimization](http://scikit-learn.org/stable/modules/grid_search.html#grid-search) because it is very flexible. You can look at different values of a hyper-parameter, different [kernels](http://scikit-learn.org/stable/modules/svm.html), different training/test split sizes, etc. The input is a dictionary where the key is the parameter of interest (the sides of the grid) and the values are the parameter increments to search over (the steps of the grid)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 3:**<a id=\"ex3\"></a> Grid search can be slow because it returns results for all possible combinations of hyper-parameters. Can you think of a more efficient way to find the good hyper-parameter settings (Hint: How can you narrow the search?)\n",
    "\n",
    "**A:**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we are going to do a grid search over the SVM cost parameter and investigate the results. The output contains information about the best hyper-parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Search over different cost parameters\n",
    "parameters = {'C':[0.01, 0.1, 1, 10]}\n",
    "clf = GridSearchCV(SVC(kernel='linear'),\n",
    "                   parameters,\n",
    "                   cv=StratifiedShuffleSplit(n_splits=3, test_size=0.1),\n",
    "                   return_train_score=True)\n",
    "clf.fit(bold_data, labels);\n",
    "\n",
    "# Print the results\n",
    "print(clf.best_estimator_)  # What was the best classifier and cost?\n",
    "print(clf.best_score_)  # What was the best classification score?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Want to see more details from the cross validation? All the results are stored in the dictionary cv\\_results\\_. Let's took a look at some of the important metrics stored here. For more details you can look at the [cv\\_results\\_ method on scikit-learn](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html).\n",
    "\n",
    "You can printout cv\\_results\\_ directly or for a nicer look you can import it into a pandas dataframe and print it out. Each row corresponds to one parameter combination.\n",
    "\n",
    "([Pandas](https://pandas.pydata.org/pandas-docs/stable/index.html) is a widely used data processing and machine learning package. Some people love it more than the animal.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Ugly way\n",
    "print(clf.cv_results_)\n",
    "print(\"\\nUh, that is not in a good human-readable format.\\n\")\n",
    "\n",
    "# Nicer way (using pandas)\n",
    "results = pd.DataFrame(clf.cv_results_)\n",
    "print(\"It's much easier to read this way, after converting it to a pandas dataframe: \\n\")\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now going to do some different types of cross-validation hyper-parameter tuning.\n",
    "\n",
    "**Exercise 4:**<a id=\"ex4\"></a> In machine learning, kernels are classes of algorithms that can be used to create a model. The (gaussian) radial basis function (RBF) kernel is very common in SVM classifiers. Briefly describe what it does:\n",
    "\n",
    "**A:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Search over different cost and gamma parameters of a radial basis kernel\n",
    "parameters = {'gamma':[10e-3, 10e0, 10e3], 'C':[10e-3, 10e0, 10e3]}\n",
    "clf = GridSearchCV(SVC(kernel='rbf'),\n",
    "                   parameters,\n",
    "                   cv=StratifiedShuffleSplit(n_splits=3, test_size=0.1))\n",
    "clf.fit(bold_data, labels)\n",
    "print(clf.best_estimator_)  # What was the best classifier and parameters?\n",
    "print(clf.best_score_)  # What was the best classification score?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 5:**<a id=\"ex5\"></a>  When would linear SVM be expected to outperform other kernels and why? Answer this question and run an analysis in which you compare linear, polynomial, and RBF kernels for SVM using GridSearchCV. This doesn't mean you run three separate GridSearchCV calls, this mean you should use these kernels as different hyper-parameters (as well as fitting cost and gamma).\n",
    "\n",
    "**A:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Insert code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we are writing a classification pipeline, nested cross validation can be very useful. As the name suggests, this procedure nests a second cross-validation within folds of the first cross validation. As before, we will divide data into training and test sets (outer loop), but additionally will divide the training set itself in order to set the hyper-parameters into training and test (or validation) sets (inner loop).\n",
    "\n",
    "Thus, on each split we now have a training (inner), validation (inner), and test (outer) dataset; a typical dataset size distribution might be 60%, 20%, 20%. Within the inner loop we train the model and find the optimal hyper-parameters (i.e., that have the highest performance when tested on the validation data). The typical practice is to then retrain your model with these hyper-parameters on both the training AND validation datasets and then evaluate on your held-out test dataset to get a score.\n",
    "\n",
    "![image](https://qph.ec.quoracdn.net/main-qimg-bb7689c141427db9ab8ab030745aa8bc)\n",
    "\n",
    "This is turtles all the way down, you could have any number of inner loops. However, you will run into data issues quickly (not enough data for training) and you will also run the risk of over-fitting your data: you will find the optimal parameters for a small set of your data but this might not generalize to the rest of your data. For more on the problem of overfitting, take a look at [this short and comprehensible EliteDataScience post](https://elitedatascience.com/overfitting-in-machine-learning).\n",
    "\n",
    "For more description and a good summary of what you have learnt so far then check [here](http://www.predictiveanalyticsworld.com/patimes/nested-cross-validation-simple-cross-validation-isnt-enough/8952/).\n",
    "\n",
    "**Self-study:** Some people discourage training on both training and validation data, saying you should only ever use the training data for fitting the model. Figure out why these people hold these views."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 6:**<a id=\"ex6\"></a> Set up a nested cross validation loop. In this loop you will perform hyper-parameter cross validation on a training dataset (which itself will be split into training and validation) and then score these optimized hyper-parameters on the test dataset. Perform 10 outer loop folds and 5 inner loop folds. Report the mean classification score for the outer loops.  \n",
    "Things to watch out for: \n",
    "- The optimal hyper-parameter settings for each outer loop fold can be different; don't have a double nested analysis (easy mistake to make if you use GridSearchCV).\n",
    "- You do not acutally need to set up a for-loop for this\n",
    "- As always: In doubt, check if the [scikit-learn documentation](http://scikit-learn.org/stable/index.html) or [StackExchange Community](https://stackexchange.com/) is helpful\n",
    "- Running the nested cross validation will take a couple of minutes. Grab a snack."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. How to avoid double dipping <a id=\"double_dipping\"></a>\n",
    "\n",
    "One of the good things about the GridSearchCV method is that it makes it easy (but not impossible!) to prevent double dipping. In previous exercises we examined cases where double dipping is clear (e.g., training on all of the data and testing on a subset); however, double dipping can be a lot more subtle and harder to detect.\n",
    "\n",
    "For instance, a common form of double dipping is Z scoring over both your training and testing datasets together, rather than Z scoring the two groups separately (in fact we are doing it in this exercise right now!). This is doubling dipping because information from one group affects the other. Imagine a scenario where you were using different runs as your test set. It might be that on a given run that the variability in activity is much higher than in all the other runs. By Z scoring over all runs you are decreasing the variability in all the other runs which could mask any patterns of variability. \n",
    "\n",
    "In practice, Z scoring can be unavoidable: if we have different runs but we don't want to use them as the basis for our training/test splits (for instance because there are practice effects) then we need to combine samples from different runs. Without normalization, these may have wildly different scales due to scanner drift or other confounds, distorting the classifier. Hence we need to normalize within run but this could be considered double dipping because each run includes both training and test data. Even without these concerns about different scales between runs, we might also worry about Z scoring over small numbers of observations in our test set. In the end, Z scoring is double dipping like jaywalking is illegal.\n",
    "\n",
    "**Self-study:** Simulate an example where double dipping with Z scoring affects the results. (Hint: make observations  that are noisy samples of a given pattern of results where the amount of noise varies).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 7:** <a id=\"ex7\"></a> If we do a 5 x 6 grid search is there a greater risk of double dipping than if we do a 3 x 2? Are there concerns with overfitting?\n",
    "\n",
    "**A:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of double dipping<a id=\"example_double_dip\"></a>\n",
    "\n",
    "Below we work through an exercise of another common type of double dipping in which we perform voxel selection on all of our data before splitting it into a training and test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_folds = 100  # How many folds of the classifier\n",
    "test_size = 0.2\n",
    "skfold = StratifiedShuffleSplit(n_splits=n_folds, test_size=test_size) \n",
    "\n",
    "clf_score = np.array([])\n",
    "for train, test in skfold.split(bold_data, labels):\n",
    "    \n",
    "    # Do voxel selection on all voxels\n",
    "    mean_threshold = np.percentile(np.mean(bold_data, axis=0), 95)\n",
    "    selected_voxels = np.where(mean_threshold <= np.mean(bold_data, axis = 0))\n",
    "    \n",
    "    # Pull out the sample data\n",
    "    train_data = bold_data[train, :]\n",
    "    test_data = bold_data[test, :]\n",
    "\n",
    "    # Train and test the classifier\n",
    "    classifier = SVC(kernel=\"linear\", C=1)\n",
    "    clf = classifier.fit(train_data[:, selected_voxels[0]], labels[train])\n",
    "    score = clf.score(test_data[:, selected_voxels[0]], labels[test])\n",
    "    clf_score = np.hstack((clf_score, score))\n",
    "\n",
    "print(clf_score.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 8:**<a id=\"ex8\"></a> Create a copy of this code that fixes the concerns about double dipping. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Insert code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Build a Pipeline <a id=\"pipeline\"></a>\n",
    "\n",
    "scikit-learn has a method, [Pipeline](http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html#sklearn.pipeline.Pipeline), that simplifies running preprocessing steps in an automated fashion. Below we create a pipeline with the following steps:\n",
    ">Scale the data.  \n",
    ">Use PCA and choose the best option from a set of dimensions.  \n",
    ">Choose the best cost hyperparameter value for an SVM.\n",
    "\n",
    "It is then really easy to do cross validation at different levels of this pipeline.\n",
    "\n",
    "The steps below are based on [this example in scikit-learn](http://scikit-learn.org/stable/auto_examples/plot_compare_reduction.html#illustration-of-pipeline-and-gridsearchcv)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "304fd1c3-80aa-4829-a36a-40c45dde562a"
    }
   },
   "outputs": [],
   "source": [
    "# Set up the pipeline\n",
    "pipe = Pipeline([\n",
    "        ('scale', preprocessing.StandardScaler()),\n",
    "        ('reduce_dim', PCA()),\n",
    "        ('classify', SVC(kernel=\"linear\")),\n",
    "    ])\n",
    "\n",
    "# PCA dimensions\n",
    "component_steps = [20, 40]\n",
    "\n",
    "# Classifier cost options\n",
    "c_steps = [10e-1, 10e0, 10e1, 10e2]\n",
    "\n",
    "# Build the grid search dictionary\n",
    "param_grid = [\n",
    "    {\n",
    "        'reduce_dim': [PCA(iterated_power=7)], \n",
    "        'reduce_dim__n_components': component_steps,\n",
    "        'classify__C': c_steps,\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to put it all together and run the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# parallelization parameter, will return to this later...\n",
    "n_jobs=1\n",
    "\n",
    "clf_pipe = GridSearchCV(pipe,\n",
    "                        cv=3,\n",
    "                        n_jobs=n_jobs,\n",
    "                        param_grid=param_grid,\n",
    "                        return_train_score=True\n",
    "                       )\n",
    "clf_pipe.fit(bold_data, labels)  # run the pipeline\n",
    "\n",
    "print(clf_pipe.best_estimator_)  # What was the best classifier and parameters?\n",
    "print()  # easy way to output a blank line to structure your output\n",
    "print(clf_pipe.best_score_)  # What was the best classification score?\n",
    "print()\n",
    "\n",
    "# sort results with declining mean test score\n",
    "cv_results = pd.DataFrame(clf_pipe.cv_results_)\n",
    "print(cv_results.sort_values(by='mean_test_score', ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 9:**<a id=\"ex9\"></a> Build a pipeline that takes the following steps:\n",
    "\n",
    "1. Z score the data.  \n",
    "2. Grid search over PCA and the VarianceThreshold method for voxel selection. In other words, test the pipeline with either PCA as your method for voxel selection or VarianceThreshold as your method for voxel selection.\n",
    "3. Grid search over the linear and RBF SVM kernel.\n",
    "\n",
    "Run this pipeline for at least 5 subjects and present your average results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Insert your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Novel contribution:**<a id=\"novel\"></a> be creative and make one new discovery by adding an analysis, visualization, or optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contributions <a id=\"contributions\"></a> \n",
    "\n",
    "M. Kumar, C. Ellis and N. Turk-Browne produced the initial notebook  \n",
    "T. Meissner minor edits"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
